{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermediate Processing\n",
    "- group overlapping dataframes into a single dataframe\n",
    "- runtime: \\< 30m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6418/3142606034.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS_VA = ['TA_F', 'SW_IN_F', 'LW_IN_F', 'VPD_F', 'PA_F', 'P_F', 'WS_F', 'WD', 'RH', 'USTAR', 'NETRAD', 'PPFD_IN', 'PPFD_DIF', 'PPFD_OUT', 'SW_DIF', 'SW_OUT', 'LW_OUT',\n",
    "        'CO2_F_MDS', 'G_F_MDS', 'LE_F_MDS', 'H_F_MDS', 'NEE_VUT_REF', 'RECO_NT_VUT_REF', 'RECO_DT_VUT_REF', 'GPP_NT_VUT_REF', 'GPP_DT_VUT_REF']\n",
    "COLS_QC = [f'{c}_QC' for c in COLS_VA]\n",
    "COLS_TS = ['TIMESTAMP_START']\n",
    "\n",
    "INTERMEDIATE_DIR = Path('data/intermediate/final_int_1')\n",
    "META_DIR = Path('data/meta')\n",
    "OUTPUT_DIR = Path('data/intermediate/final_int_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Combining observations from different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRIORITIES = {\n",
    "    'ameriflux': 1,\n",
    "    'icos-2023': 1,\n",
    "    'icos-ww': 2,\n",
    "    'fluxnet': 3\n",
    "}\n",
    "\n",
    "def add_hour(timestamp):\n",
    "    year = timestamp[0:4]\n",
    "    month = timestamp[4:6]\n",
    "    day = timestamp[6:8]\n",
    "    hour = timestamp[8:]\n",
    "    if month == '12' and day == '31':\n",
    "        return f'{str(int(year)+1)}01010000'\n",
    "    else:\n",
    "        return timestamp\n",
    "\n",
    "\n",
    "def has_overlap(data_1, data_2):\n",
    "    start_1 = str(data_1[1])\n",
    "    end_1 = add_hour(str(data_1[2]))\n",
    "    start_2 = str(data_2[1])\n",
    "    end_2 = add_hour(str(data_2[2]))\n",
    "    disjoint_before = end_1 < start_2\n",
    "    disjoint_after = start_1 > end_2\n",
    "    return not disjoint_before and not disjoint_after\n",
    "\n",
    "\n",
    "def merge_sites(data_1, data_2):\n",
    "    # implement\n",
    "    df1 = data_1[0]\n",
    "    df2 = data_2[0]\n",
    "    merged_df = pd.merge(df1, df2, on='TIMESTAMP_START', how='outer', suffixes=('_df1', '_df2'))\n",
    "\n",
    "    # If QC value is better in one than the other, set null.\n",
    "    for col in COLS_VA:\n",
    "        c1 = f'{col}_df1'\n",
    "        c2 = f'{col}_df2'\n",
    "        qc1 = f'{col}_QC_df1'\n",
    "        qc2 = f'{col}_QC_df2'\n",
    "\n",
    "        merged_df[qc1] = merged_df[qc1].fillna(5)\n",
    "        merged_df[qc2] = merged_df[qc2].fillna(5)\n",
    "        qcidx = merged_df[qc1]-merged_df[qc2]\n",
    "        # at this point, qcidx <= 0 means keep df1 value. qcidx > 0 means keep df2 value.\n",
    "\n",
    "        merged_df.loc[qcidx > 0.0, c1] = np.nan\n",
    "        merged_df[col] = merged_df[c1].combine_first(merged_df[c2])\n",
    "        merged_df[f'{col}_QC'] = merged_df[[qc1, qc2]].min(axis=1)\n",
    "        merged_df.loc[merged_df[f'{col}_QC'] == 5, f'{col}_QC'] = np.nan\n",
    "        merged_df = merged_df.drop(labels=[c1, c2, qc1, qc2], axis=1).sort_values(by='TIMESTAMP_START')\n",
    "    return (merged_df, merged_df.index[0], merged_df.index[-1].max(), f'{data_1[-1]},{data_2[-1]}')\n",
    "\n",
    "\n",
    "def merge_site_data(site_data):\n",
    "    all_merged = False\n",
    "    while not all_merged:\n",
    "        merged_sites = []\n",
    "        all_merged = True\n",
    "        i = 0\n",
    "        while i < len(site_data):\n",
    "            if i == len(site_data)-1:\n",
    "                merged_sites.append(site_data[i])\n",
    "                i += 1\n",
    "            elif has_overlap(site_data[i], site_data[i+1]):\n",
    "                all_merged = False\n",
    "                new_data = merge_sites(site_data[i], site_data[i+1])\n",
    "                merged_sites.append(new_data)\n",
    "                i += 2\n",
    "            else:\n",
    "                merged_sites.append(site_data[i])\n",
    "                i += 1\n",
    "\n",
    "        site_data = merged_sites\n",
    "    return site_data\n",
    "\n",
    "\n",
    "def process_unmerged_site_data(site, csv_info):\n",
    "    site_out_dir = os.path.join(OUTPUT_DIR, site)\n",
    "    if not os.path.exists(site_out_dir):\n",
    "        os.makedirs(site_out_dir)\n",
    "    \n",
    "    files = [file for _, file in csv_info]\n",
    "    site_data = []\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file)\n",
    "        df = df.set_index('TIMESTAMP_START')\n",
    "        filename = file.stem\n",
    "        start, end, source = filename.split('_')\n",
    "        start = int(start)\n",
    "        end = int(end)\n",
    "        site_data.append((df, start, end, source))\n",
    "    site_data = sorted(site_data, key=lambda x: PRIORITIES[x[-1]])\n",
    "    \n",
    "    if len(site_data) > 1:\n",
    "        site_data = merge_site_data(site_data)\n",
    "\n",
    "    for site in site_data:\n",
    "        if not os.path.exists(os.path.join(site_out_dir, f'{site[1]}_{site[2]}_{site[3]}')):\n",
    "            os.makedirs(os.path.join(site_out_dir, f'{site[1]}_{site[2]}_{site[3]}'))\n",
    "        site[0].to_csv(os.path.join(site_out_dir, f'{site[1]}_{site[2]}_{site[3]}', 'data.csv'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 386/386 [10:15<00:00,  1.59s/it]\n"
     ]
    }
   ],
   "source": [
    "sites = defaultdict(list)\n",
    "for source in os.listdir(INTERMEDIATE_DIR):\n",
    "    for site in os.listdir(INTERMEDIATE_DIR / source):\n",
    "        if site == 'site_data.csv':\n",
    "            continue\n",
    "        site_file = os.listdir(INTERMEDIATE_DIR / source / site)[0]\n",
    "        sites[site].append((source, INTERMEDIATE_DIR / source / site / site_file))\n",
    "\n",
    "for site, csv_info in tqdm(sites.items()):\n",
    "    process_unmerged_site_data(site, csv_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get new metadata\n",
    "meta_dfs = [pd.read_csv(INTERMEDIATE_DIR / source / 'site_data.csv') for source in os.listdir(INTERMEDIATE_DIR)]\n",
    "\n",
    "def convert_time(timestep_string):\n",
    "    return f'{timestep_string[0:4]}_{timestep_string[4:6]}_{timestep_string[6:8]}'\n",
    "\n",
    "meta_df = pd.concat(meta_dfs, axis=0)[['SITE_ID', 'LOCATION_LAT', 'LOCATION_LON', 'LOCATION_ELEV', 'IGBP']].drop_duplicates('SITE_ID')\n",
    "sites = os.listdir(OUTPUT_DIR)\n",
    "meta_df = meta_df[meta_df['SITE_ID'].isin(sites)]\n",
    "meta_df['TIME_INFO'] = pd.NA\n",
    "for site in sites:\n",
    "    subsets = os.listdir(os.path.join(OUTPUT_DIR, site))\n",
    "    time_info = {s.split('_')[2]: [convert_time(s.split('_')[0]), convert_time(s.split('_')[1])] for s in subsets}\n",
    "    filtered_rows = meta_df['SITE_ID'] == site\n",
    "    meta_df.loc[filtered_rows, 'TIME_INFO'] = [time_info] * sum(filtered_rows)\n",
    "\n",
    "\n",
    "if not os.path.exists(META_DIR):\n",
    "    os.makedirs(META_DIR)\n",
    "meta_df.to_csv(META_DIR / 'processed_site_meta.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermediate Processing\n",
    "- replace sentinel values with NaN\n",
    "- downsample from half-hourly to hourly (if needed for each site)\n",
    "- group overlapping dataframes into a single dataframe\n",
    "- runtime: \\< 30m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS_VA = ['TA_F', 'SW_IN_F', 'LW_IN_F', 'VPD_F', 'PA_F', 'P_F', 'WS_F', 'WD', 'RH', 'USTAR', 'NETRAD', 'PPFD_IN', 'PPFD_DIF', 'PPFD_OUT', 'SW_DIF', 'SW_OUT', 'LW_OUT',\n",
    "        'CO2_F_MDS', 'G_F_MDS', 'LE_F_MDS', 'H_F_MDS', 'NEE_VUT_REF', 'RECO_NT_VUT_REF', 'RECO_DT_VUT_REF', 'GPP_NT_VUT_REF', 'GPP_DT_VUT_REF']\n",
    "COLS_QC = [f'{c}_QC' for c in COLS_VA]\n",
    "COLS_TS = ['TIMESTAMP_START']\n",
    "\n",
    "BASE_DIR = 'data/raw'\n",
    "ameriflux_dir = os.path.join(BASE_DIR, 'ameriflux', 'unzipped')\n",
    "fluxnet_dir = os.path.join(BASE_DIR, 'fluxnet', 'unzipped')\n",
    "icos_ww_dir = os.path.join(BASE_DIR, 'icos-ww', 'unzipped')\n",
    "icos_2023_dir = os.path.join(BASE_DIR, 'icos-2023', 'unzipped')\n",
    "\n",
    "INTERMEDIATE_DIR = 'data/intermediate/int_1'\n",
    "OUTPUT_DIR = 'data/intermediate/int_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_site_dataframe(df, downsample=True):\n",
    "    df = df.replace(-9999.0, np.nan)\n",
    "    for column in COLS_VA + COLS_QC:\n",
    "        if column not in df.columns:\n",
    "            df[column] = np.nan\n",
    "    \n",
    "    df_ts = df[COLS_TS]\n",
    "    df_va = df[COLS_VA]\n",
    "    df_qc = df[COLS_QC]\n",
    "\n",
    "    if downsample:\n",
    "        # Average to hourly data\n",
    "        grouping_key = np.arange(len(df_va)) // 2\n",
    "        df_va = df_va.groupby(grouping_key).mean().reset_index(drop=True)\n",
    "        df_ts = df_ts.iloc[::2,:].reset_index(drop=True)\n",
    "        df_qc = df_qc.iloc[::2,:].reset_index(drop=True)\n",
    "    \n",
    "    # Double precipitation, as this should not be averaged\n",
    "    df_va['P_F'] = df_va['P_F'] * 2.0\n",
    "    df = pd.concat([df_ts, df_va, df_qc], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of (site, file, needs_downsampling)\n",
    "data = []\n",
    "\n",
    "ameriflux_sites = os.listdir(ameriflux_dir)\n",
    "for site in ameriflux_sites:\n",
    "    files = os.listdir(os.path.join(ameriflux_dir, site))\n",
    "    fluxnet_compatible_files = [f for f in files if 'FLUXNET_SUBSET_HH' in f  and 'VARINFO' not in f]\n",
    "    if len(fluxnet_compatible_files) == 1:\n",
    "        data.append((site, os.path.join(ameriflux_dir, site, fluxnet_compatible_files[0]), 'ameriflux', True))\n",
    "    else:\n",
    "        fluxnet_compatible_files = [f for f in files if 'FLUXNET_SUBSET_HR' in f  and 'VARINFO' not in f]\n",
    "        if len(fluxnet_compatible_files) != 1:\n",
    "            print(f'No valid file found for {site}')\n",
    "            continue\n",
    "        data.append((site, os.path.join(ameriflux_dir, site, fluxnet_compatible_files[0]), 'ameriflux', False))\n",
    "\n",
    "fluxnet_sites = os.listdir(fluxnet_dir)\n",
    "for site in fluxnet_sites:\n",
    "    file = os.listdir(os.path.join(fluxnet_dir, site))[0]\n",
    "    data.append((site, os.path.join(fluxnet_dir, site, file), 'fluxnet', True))\n",
    "\n",
    "icos_ww_sites = os.listdir(icos_ww_dir)\n",
    "for site in icos_ww_sites:\n",
    "    files = os.listdir(os.path.join(icos_ww_dir, site))\n",
    "    fluxnet_compatible_files = [f for f in files if 'FLUXNET2015_FULLSET_HH' in f and 'VARINFO' not in f]\n",
    "    if len(fluxnet_compatible_files) != 1:\n",
    "        continue\n",
    "    file = fluxnet_compatible_files[0]\n",
    "    data.append((site, os.path.join(icos_ww_dir, site, file), 'icos-ww', True))\n",
    "\n",
    "icos_2023_sites = os.listdir(icos_2023_dir)\n",
    "for site in icos_2023_sites:\n",
    "    files = os.listdir(os.path.join(icos_2023_dir, site))\n",
    "    fluxnet_compatible_files = [f for f in files if 'FLUXNET_HH_L2' in f and 'VARINFO' not in f]\n",
    "    if len(fluxnet_compatible_files) != 1:\n",
    "        continue\n",
    "    file = fluxnet_compatible_files[0]\n",
    "    data.append((site, os.path.join(icos_2023_dir, site, file), 'icos-2023', True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(INTERMEDIATE_DIR):\n",
    "    os.makedirs(INTERMEDIATE_DIR)\n",
    "\n",
    "for site, file, source, downsample in data:\n",
    "    print(f'Processing {file}...')\n",
    "    site_dir = os.path.join(INTERMEDIATE_DIR, site)\n",
    "    if not os.path.exists(site_dir):\n",
    "        os.makedirs(site_dir)\n",
    "    \n",
    "    site_df = pd.read_csv(file)\n",
    "    processed_df = process_site_dataframe(site_df, downsample=downsample)\n",
    "    min_time = processed_df['TIMESTAMP_START'].min()\n",
    "    max_time = processed_df['TIMESTAMP_START'].max()\n",
    "    outfile = os.path.join(site_dir, f'{min_time}_{max_time}_{source}.csv')\n",
    "    processed_df.to_csv(outfile, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Combining observations from different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRIORITIES = {\n",
    "    'ameriflux': 1,\n",
    "    'icos-2023': 1,\n",
    "    'icos-ww': 2,\n",
    "    'fluxnet': 3\n",
    "}\n",
    "\n",
    "def add_hour(timestamp):\n",
    "    year = timestamp[0:4]\n",
    "    month = timestamp[4:6]\n",
    "    day = timestamp[6:8]\n",
    "    hour = timestamp[8:]\n",
    "    if month == '12' and day == '31':\n",
    "        return f'{str(int(year)+1)}01010000'\n",
    "    else:\n",
    "        return timestamp\n",
    "\n",
    "\n",
    "def has_overlap(data_1, data_2):\n",
    "    start_1 = str(data_1[1])\n",
    "    end_1 = add_hour(str(data_1[2]))\n",
    "    start_2 = str(data_2[1])\n",
    "    end_2 = add_hour(str(data_2[2]))\n",
    "    disjoint_before = end_1 < start_2\n",
    "    disjoint_after = start_1 > end_2\n",
    "    return not disjoint_before and not disjoint_after\n",
    "\n",
    "\n",
    "def merge_sites(data_1, data_2):\n",
    "    # implement\n",
    "    df1 = data_1[0]\n",
    "    df2 = data_2[0]\n",
    "    merged_df = pd.merge(df1, df2, on='TIMESTAMP_START', how='outer', suffixes=('_df1', '_df2'))\n",
    "\n",
    "    # If QC value is better in one than the other, set null.\n",
    "    for col in COLS_VA:\n",
    "        c1 = f'{col}_df1'\n",
    "        c2 = f'{col}_df2'\n",
    "        qc1 = f'{col}_QC_df1'\n",
    "        qc2 = f'{col}_QC_df2'\n",
    "\n",
    "        merged_df[qc1] = merged_df[qc1].fillna(5)\n",
    "        merged_df[qc2] = merged_df[qc2].fillna(5)\n",
    "        qcidx = merged_df[qc1]-merged_df[qc2]\n",
    "        # at this point, qcidx <= 0 means keep df1 value. qcidx > 0 means keep df2 value.\n",
    "\n",
    "        merged_df.loc[qcidx > 0.0, c1] = np.nan\n",
    "        merged_df[col] = merged_df[c1].combine_first(merged_df[c2])\n",
    "        merged_df[f'{col}_QC'] = merged_df[[qc1, qc2]].min(axis=1)\n",
    "        merged_df.loc[merged_df[f'{col}_QC'] == 5, f'{col}_QC'] = np.nan\n",
    "        merged_df = merged_df.drop(labels=[c1, c2, qc1, qc2], axis=1).sort_values(by='TIMESTAMP_START')\n",
    "    return (merged_df, merged_df.index[0], merged_df.index[-1].max(), f'{data_1[-1]},{data_2[-1]}')\n",
    "\n",
    "\n",
    "def merge_site_data(site_data):\n",
    "    all_merged = False\n",
    "    while not all_merged:\n",
    "        merged_sites = []\n",
    "        all_merged = True\n",
    "        i = 0\n",
    "        while i < len(site_data):\n",
    "            if i == len(site_data)-1:\n",
    "                merged_sites.append(site_data[i])\n",
    "                i += 1\n",
    "            elif has_overlap(site_data[i], site_data[i+1]):\n",
    "                all_merged = False\n",
    "                new_data = merge_sites(site_data[i], site_data[i+1])\n",
    "                merged_sites.append(new_data)\n",
    "                i += 2\n",
    "            else:\n",
    "                merged_sites.append(site_data[i])\n",
    "                i += 1\n",
    "\n",
    "        site_data = merged_sites\n",
    "    return site_data\n",
    "\n",
    "\n",
    "def process_unmerged_site_data(site, in_dir, out_dir):\n",
    "    print(f'Processing {site}...')\n",
    "    site_out_dir = os.path.join(out_dir, site)\n",
    "    if not os.path.exists(site_out_dir):\n",
    "        os.makedirs(site_out_dir)\n",
    "    \n",
    "    site_in_dir = os.path.join(in_dir, site)\n",
    "    files = os.listdir(site_in_dir)\n",
    "    site_data = []\n",
    "    for file in files:\n",
    "        df = pd.read_csv(os.path.join(site_in_dir, file))\n",
    "        df = df.set_index('TIMESTAMP_START')\n",
    "        filename = file.split('.')[0]\n",
    "        start, end, source = filename.split('_')\n",
    "        start = int(start)\n",
    "        end = int(end)\n",
    "        site_data.append((df, start, end, source))\n",
    "    site_data = sorted(site_data, key=lambda x: PRIORITIES[x[-1]])\n",
    "    \n",
    "    if len(site_data) > 1:\n",
    "        site_data = merge_site_data(site_data)\n",
    "\n",
    "    for site in site_data:\n",
    "        if not os.path.exists(os.path.join(site_out_dir, f'{site[1]}_{site[2]}_{site[3]}')):\n",
    "            os.makedirs(os.path.join(site_out_dir, f'{site[1]}_{site[2]}_{site[3]}'))\n",
    "        site[0].to_csv(os.path.join(site_out_dir, f'{site[1]}_{site[2]}_{site[3]}', 'data.csv'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing US-HB3...\n",
      "Processing CA-Gro...\n",
      "Processing AR-TF1...\n",
      "Processing DE-Akm...\n",
      "Processing US-Mo1...\n",
      "Processing US-NR1...\n",
      "Processing BE-Vie...\n",
      "Processing US-UM3...\n",
      "Processing CG-Tch...\n",
      "Processing IT-MBo...\n",
      "Processing AU-Dry...\n",
      "Processing US-MOz...\n",
      "Processing US-NC3...\n",
      "Processing CH-Aws...\n",
      "Processing FR-FBn...\n",
      "Processing US-CS1...\n",
      "Processing AT-Neu...\n",
      "Processing US-KS2...\n",
      "Processing US-StJ...\n",
      "Processing AR-Vir...\n",
      "Processing SE-Ros...\n",
      "Processing US-xDL...\n",
      "Processing IT-Isp...\n",
      "Processing CA-SF2...\n",
      "Processing US-GBT...\n",
      "Processing ZM-Mon...\n",
      "Processing US-Jo1...\n",
      "Processing US-CS3...\n",
      "Processing CA-Ca2...\n",
      "Processing US-Tw4...\n",
      "Processing BE-Maa...\n",
      "Processing DE-SfN...\n",
      "Processing US-Rwf...\n",
      "Processing US-UMB...\n",
      "Processing US-Ro6...\n",
      "Processing CH-Cha...\n",
      "Processing CZ-KrP...\n",
      "Processing DK-Sor...\n",
      "Processing CH-Oe2...\n",
      "Processing BR-Sa3...\n",
      "Processing AU-Wac...\n",
      "Processing CA-ARF...\n",
      "Processing DE-Lnf...\n",
      "Processing CA-Ca1...\n",
      "Processing CN-Dan...\n",
      "Processing DE-Lkb...\n",
      "Processing CH-Dav...\n",
      "Processing US-IB2...\n",
      "Processing CN-Du2...\n",
      "Processing IT-La2...\n",
      "Processing IE-Cra...\n",
      "Processing DE-Har...\n",
      "Processing US-A74...\n",
      "Processing ES-Agu...\n",
      "Processing US-xDJ...\n",
      "Processing US-BRG...\n",
      "Processing GL-Dsk...\n",
      "Processing US-Rls...\n",
      "Processing IT-BCi...\n",
      "Processing US-Jo2...\n",
      "Processing US-SRS...\n",
      "Processing CA-NS3...\n",
      "Processing FR-Gri...\n",
      "Processing US-RGB...\n",
      "Processing US-Srr...\n",
      "Processing AU-Whr...\n",
      "Processing US-KS1...\n",
      "Processing CA-MA3...\n",
      "Processing US-Var...\n",
      "Processing CZ-wet...\n",
      "Processing US-ORv...\n",
      "Processing IT-SR2...\n",
      "Processing US-ONA...\n",
      "Processing CA-NS5...\n",
      "Processing DE-RuR...\n",
      "Processing US-BZS...\n",
      "Processing US-Hn2...\n",
      "Processing IT-Cp2...\n",
      "Processing US-Wi3...\n",
      "Processing US-Ses...\n",
      "Processing US-Goo...\n",
      "Processing US-Pnp...\n",
      "Processing US-Wi5...\n",
      "Processing US-Ho2...\n",
      "Processing CH-Oe1...\n",
      "Processing US-AR1...\n",
      "Processing ES-LgS...\n",
      "Processing DE-Msr...\n",
      "Processing US-Ne1...\n",
      "Processing US-xUN...\n",
      "Processing BE-Bra...\n",
      "Processing IT-Tor...\n",
      "Processing AU-ASM...\n",
      "Processing US-xMB...\n",
      "Processing US-xML...\n",
      "Processing IL-Yat...\n",
      "Processing SD-Dem...\n",
      "Processing FI-Jok...\n",
      "Processing BE-Lcr...\n",
      "Processing US-Seg...\n",
      "Processing RU-Fyo...\n",
      "Processing FI-Hyy...\n",
      "Processing DE-Geb...\n",
      "Processing BE-Lon...\n",
      "Processing US-Wi9...\n",
      "Processing US-DS3...\n",
      "Processing FI-Lom...\n",
      "Processing CA-TP3...\n",
      "Processing FR-Fon...\n",
      "Processing US-xTR...\n",
      "Processing US-xUK...\n",
      "Processing CA-Oas...\n",
      "Processing US-ICh...\n",
      "Processing US-ARM...\n",
      "Processing US-A32...\n",
      "Processing US-Fcr...\n",
      "Processing DE-RuW...\n",
      "Processing US-HB1...\n",
      "Processing PE-QFR...\n",
      "Processing US-CF1...\n",
      "Processing AU-TTE...\n",
      "Processing US-xBA...\n",
      "Processing CA-Obs...\n",
      "Processing IT-CA1...\n",
      "Processing AU-Ync...\n",
      "Processing DE-Zrk...\n",
      "Processing ES-LJu...\n",
      "Processing IT-Lav...\n",
      "Processing FI-Let...\n",
      "Processing US-Tw1...\n",
      "Processing US-ARb...\n",
      "Processing IT-Ro2...\n",
      "Processing IT-Col...\n",
      "Processing US-Ro2...\n",
      "Processing US-Fmf...\n",
      "Processing PA-SPs...\n",
      "Processing US-xJE...\n",
      "Processing US-Me3...\n",
      "Processing GL-NuF...\n",
      "Processing US-GLE...\n",
      "Processing US-EDN...\n",
      "Processing UK-AMo...\n",
      "Processing CA-TP2...\n",
      "Processing IT-Lsn...\n",
      "Processing CN-Cng...\n",
      "Processing US-NC1...\n",
      "Processing US-Wi4...\n",
      "Processing US-Tw2...\n",
      "Processing US-Cop...\n",
      "Processing US-xTA...\n",
      "Processing SE-Deg...\n",
      "Processing US-Me2...\n",
      "Processing US-xSL...\n",
      "Processing AU-Ade...\n",
      "Processing US-Ton...\n",
      "Processing IT-CA3...\n",
      "Processing US-Wi0...\n",
      "Processing CH-Lae...\n",
      "Processing US-xJR...\n",
      "Processing MX-Tes...\n",
      "Processing US-DFC...\n",
      "Processing CA-NS6...\n",
      "Processing CH-Fru...\n",
      "Processing US-Bar...\n",
      "Processing CA-TP4...\n",
      "Processing US-NC4...\n",
      "Processing IT-BFt...\n",
      "Processing US-xCL...\n",
      "Processing US-Twt...\n",
      "Processing FI-Sii...\n",
      "Processing CA-NS2...\n",
      "Processing NL-Loo...\n",
      "Processing US-Ivo...\n",
      "Processing RU-Cok...\n",
      "Processing US-CF2...\n",
      "Processing US-Wi8...\n",
      "Processing GF-Guy...\n",
      "Processing BR-CST...\n",
      "Processing US-ARc...\n",
      "Processing IT-Noe...\n",
      "Processing US-xGR...\n",
      "Processing US-WPT...\n",
      "Processing CA-Qfo...\n",
      "Processing US-Rpf...\n",
      "Processing DK-Gds...\n",
      "Processing AU-Cum...\n",
      "Processing SE-Sto...\n",
      "Processing ES-Abr...\n",
      "Processing US-Wjs...\n",
      "Processing US-EML...\n",
      "Processing US-xNQ...\n",
      "Processing US-BZF...\n",
      "Processing US-Rms...\n",
      "Processing US-RGo...\n",
      "Processing CA-LP1...\n",
      "Processing ES-Cnd...\n",
      "Processing US-CS4...\n",
      "Processing DK-Skj...\n",
      "Processing US-Tw5...\n",
      "Processing CD-Ygb...\n",
      "Processing CA-NS4...\n",
      "Processing US-SRM...\n",
      "Processing US-Bi1...\n",
      "Processing US-Ro4...\n",
      "Processing BR-Npw...\n",
      "Processing US-xCP...\n",
      "Processing US-KFS...\n",
      "Processing CA-MA2...\n",
      "Processing AU-How...\n",
      "Processing US-Ha1...\n",
      "Processing US-Ro5...\n",
      "Processing AU-Fog...\n",
      "Processing US-Whs...\n",
      "Processing FI-Var...\n",
      "Processing US-HB2...\n",
      "Processing CZ-BK1...\n",
      "Processing SE-Svb...\n",
      "Processing US-Wi1...\n",
      "Processing CN-Sw2...\n",
      "Processing DE-Hzd...\n",
      "Processing US-xSC...\n",
      "Processing AU-Rob...\n",
      "Processing FI-Qvd...\n",
      "Processing US-xSE...\n",
      "Processing US-LWW...\n",
      "Processing US-AR2...\n",
      "Processing FR-Hes...\n",
      "Processing US-Wi7...\n",
      "Processing SN-Dhr...\n",
      "Processing CN-HaM...\n",
      "Processing US-xBL...\n",
      "Processing US-Fuf...\n",
      "Processing US-WCr...\n",
      "Processing CA-DB2...\n",
      "Processing US-Me4...\n",
      "Processing US-xBN...\n",
      "Processing US-xKA...\n",
      "Processing AU-GWW...\n",
      "Processing RU-Fy2...\n",
      "Processing DE-Obe...\n",
      "Processing AU-DaS...\n",
      "Processing US-xKZ...\n",
      "Processing FR-Pue...\n",
      "Processing DE-Seh...\n",
      "Processing CA-ARB...\n",
      "Processing US-CS2...\n",
      "Processing ES-Ln2...\n",
      "Processing IT-Ren...\n",
      "Processing IT-SRo...\n",
      "Processing US-BZB...\n",
      "Processing US-Me5...\n",
      "Processing US-xDS...\n",
      "Processing US-Kon...\n",
      "Processing GH-Ank...\n",
      "Processing US-Prr...\n",
      "Processing US-xSB...\n",
      "Processing CN-Ha2...\n",
      "Processing AU-Cpr...\n",
      "Processing SE-Htm...\n",
      "Processing US-CF4...\n",
      "Processing FR-Lam...\n",
      "Processing CZ-Stn...\n",
      "Processing CA-NS7...\n",
      "Processing IT-Niv...\n",
      "Processing US-Wi6...\n",
      "Processing FI-Sod...\n",
      "Processing CA-Man...\n",
      "Processing AU-Stp...\n",
      "Processing US-xST...\n",
      "Processing US-UMd...\n",
      "Processing ES-LM2...\n",
      "Processing MY-PSO...\n",
      "Processing FR-Tou...\n",
      "Processing NL-Hor...\n",
      "Processing US-CRT...\n",
      "Processing US-Atq...\n",
      "Processing FR-LBr...\n",
      "Processing US-Vcm...\n",
      "Processing US-SRG...\n",
      "Processing US-xBR...\n",
      "Processing US-Mo2...\n",
      "Processing US-Me1...\n",
      "Processing US-CF3...\n",
      "Processing CA-ER1...\n",
      "Processing RU-Ha1...\n",
      "Processing IT-CA2...\n",
      "Processing US-Sta...\n",
      "Processing CA-SF3...\n",
      "Processing US-Rwe...\n",
      "Processing CA-NS1...\n",
      "Processing DE-RuS...\n",
      "Processing CN-Din...\n",
      "Processing AU-Lox...\n",
      "Processing US-xAE...\n",
      "Processing US-Blo...\n",
      "Processing JP-MBF...\n",
      "Processing SE-Nor...\n",
      "Processing US-Sne...\n",
      "Processing CA-MA1...\n",
      "Processing ES-Amo...\n",
      "Processing US-Tw3...\n",
      "Processing US-Lin...\n",
      "Processing US-Oho...\n",
      "Processing US-xSR...\n",
      "Processing DE-Spw...\n",
      "Processing US-LS2...\n",
      "Processing FR-Bil...\n",
      "Processing CA-DBB...\n",
      "Processing US-xYE...\n",
      "Processing IT-Ro1...\n",
      "Processing AU-Wom...\n",
      "Processing US-KS3...\n",
      "Processing US-OWC...\n",
      "Processing US-Rws...\n",
      "Processing US-ICs...\n",
      "Processing CN-Cha...\n",
      "Processing RU-Che...\n",
      "Processing DE-Kli...\n",
      "Processing JP-SMF...\n",
      "Processing AU-Rig...\n",
      "Processing IT-PT1...\n",
      "Processing US-Vcp...\n",
      "Processing US-xDC...\n",
      "Processing SJ-Adv...\n",
      "Processing DE-Gri...\n",
      "Processing US-xRM...\n",
      "Processing GL-ZaF...\n",
      "Processing US-NGB...\n",
      "Processing US-xHA...\n",
      "Processing AU-Gin...\n",
      "Processing US-RGA...\n",
      "Processing US-KLS...\n",
      "Processing CN-Du3...\n",
      "Processing US-xHE...\n",
      "Processing US-xAB...\n",
      "Processing CN-Qia...\n",
      "Processing US-Mpj...\n",
      "Processing US-Me6...\n",
      "Processing AU-DaP...\n",
      "Processing GL-ZaH...\n",
      "Processing AU-RDF...\n",
      "Processing US-SRC...\n",
      "Processing US-Bi2...\n",
      "Processing CA-CF1...\n",
      "Processing CZ-Lnz...\n",
      "Processing CA-SF1...\n",
      "Processing US-Hn3...\n",
      "Processing FR-Aur...\n",
      "Processing AU-Emr...\n",
      "Processing FR-LGt...\n",
      "Processing US-MMS...\n",
      "Processing IT-Cpz...\n",
      "Processing CA-TPD...\n",
      "Processing US-Snf...\n",
      "Processing CA-TP1...\n",
      "Processing US-Wkg...\n",
      "Processing DE-Hai...\n",
      "Processing DK-Eng...\n",
      "Processing US-xWD...\n",
      "Processing US-Ro1...\n",
      "Processing US-NGC...\n",
      "Processing US-ALQ...\n",
      "Processing US-Myb...\n",
      "Processing AR-SLu...\n",
      "Processing DK-Fou...\n",
      "Processing DE-Tha...\n",
      "Processing FR-Mej...\n",
      "Processing US-Los...\n",
      "Processing BE-Dor...\n",
      "Processing FI-Ken...\n",
      "Processing CA-Cbo...\n",
      "Processing US-Mo3...\n",
      "Processing DE-HoH...\n",
      "Processing US-ICt...\n",
      "Processing DK-Vng...\n",
      "Processing US-Wi2...\n",
      "Processing ES-LM1...\n",
      "Processing FR-EM2...\n",
      "Processing US-Syv...\n",
      "Processing CZ-RAJ...\n",
      "Processing CZ-BK2...\n",
      "Processing PA-SPn...\n",
      "Processing US-xNG...\n",
      "Processing US-BZo...\n",
      "Processing US-HWB...\n",
      "Processing US-xSJ...\n"
     ]
    }
   ],
   "source": [
    "sites = os.listdir(INTERMEDIATE_DIR)\n",
    "\n",
    "for site in sites:\n",
    "    process_unmerged_site_data(site, INTERMEDIATE_DIR, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get new metadata\n",
    "\n",
    "ameriflux_meta_file = os.path.join(BASE_DIR, 'ameriflux', 'site_data.csv')\n",
    "fluxnet_meta_file = os.path.join(BASE_DIR, 'fluxnet', 'site_data.csv')\n",
    "icos_ww_meta_file = os.path.join(BASE_DIR, 'icos-ww', 'site_data.csv')\n",
    "icos_2023_meta_file = os.path.join(BASE_DIR, 'icos-2023', 'site_data.csv')\n",
    "\n",
    "meta_dfs = [\n",
    "    pd.read_csv(ameriflux_meta_file),\n",
    "    pd.read_csv(fluxnet_meta_file),\n",
    "    pd.read_csv(icos_ww_meta_file),\n",
    "    pd.read_csv(icos_2023_meta_file),\n",
    "]\n",
    "\n",
    "def convert_time(timestep_string):\n",
    "    return f'{timestep_string[0:4]}_{timestep_string[4:6]}_{timestep_string[6:8]}'\n",
    "\n",
    "meta_df = pd.concat(meta_dfs, axis=0)[['SITE_ID', 'LOCATION_LAT', 'LOCATION_LON', 'LOCATION_ELEV', 'IGBP']].drop_duplicates('SITE_ID')\n",
    "sites = os.listdir(OUTPUT_DIR)\n",
    "meta_df = meta_df[meta_df['SITE_ID'].isin(sites)]\n",
    "meta_df['TIME_INFO'] = pd.NA\n",
    "for site in sites:\n",
    "    subsets = os.listdir(os.path.join(OUTPUT_DIR, site))\n",
    "    time_info = {s.split('_')[2]: [convert_time(s.split('_')[0]), convert_time(s.split('_')[1])] for s in subsets}\n",
    "    filtered_rows = meta_df['SITE_ID'] == site\n",
    "    meta_df.loc[filtered_rows, 'TIME_INFO'] = [time_info] * sum(filtered_rows)\n",
    "\n",
    "meta_df.to_csv('processed_site_meta.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

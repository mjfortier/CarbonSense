{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process ICOS-2023 data\n",
    "- replace sentinel values with NaN\n",
    "- downsample from half-hourly to hourly (if needed for each site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS_VA = ['TA_F', 'SW_IN_F', 'LW_IN_F', 'VPD_F', 'PA_F', 'P_F', 'WS_F', 'WD', 'RH', 'USTAR', 'NETRAD', 'PPFD_IN', 'PPFD_DIF', 'PPFD_OUT', 'SW_DIF', 'SW_OUT', 'LW_OUT',\n",
    "        'CO2_F_MDS', 'G_F_MDS', 'LE_F_MDS', 'H_F_MDS', 'NEE_VUT_REF', 'RECO_NT_VUT_REF', 'RECO_DT_VUT_REF', 'GPP_NT_VUT_REF', 'GPP_DT_VUT_REF']\n",
    "COLS_QC = [f'{c}_QC' for c in COLS_VA]\n",
    "COLS_TS = ['TIMESTAMP_START']\n",
    "\n",
    "collection = 'icos-2023'\n",
    "\n",
    "INPUT_DIR = os.path.join('data', 'raw', collection, 'unzipped')\n",
    "META_FILE = os.path.join('data', 'raw', collection, 'site_data.csv')\n",
    "OUTPUT_DIR = os.path.join('data', 'intermediate', 'test_int_1', collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_site_dataframe(df, downsample=True):\n",
    "    df = df.replace(-9999.0, np.nan)\n",
    "    for column in COLS_VA + COLS_QC:\n",
    "        if column not in df.columns:\n",
    "            df[column] = np.nan\n",
    "    \n",
    "    df_ts = df[COLS_TS]\n",
    "    df_va = df[COLS_VA]\n",
    "    df_qc = df[COLS_QC]\n",
    "\n",
    "    if downsample:\n",
    "        # Average to hourly data\n",
    "        grouping_key = np.arange(len(df_va)) // 2\n",
    "        df_va = df_va.groupby(grouping_key).mean().reset_index(drop=True)\n",
    "        df_ts = df_ts.iloc[::2,:].reset_index(drop=True)\n",
    "        df_qc = df_qc.iloc[::2,:].reset_index(drop=True)\n",
    "    \n",
    "    # Double precipitation, as this should not be averaged\n",
    "    df_va['P_F'] = df_va['P_F'] * 2.0\n",
    "    df = pd.concat([df_ts, df_va, df_qc], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: No compatible file found for DE-BeR\n",
      "ERROR: No compatible file found for FI-Kvr\n",
      "ERROR: No compatible file found for FI-Kmp\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for site in os.listdir(INPUT_DIR):\n",
    "    files = os.listdir(os.path.join(INPUT_DIR, site))\n",
    "    fluxnet_compatible_files = [f for f in files if 'FLUXNET_HH_L2' in f and 'VARINFO' not in f]\n",
    "    if len(fluxnet_compatible_files) != 1:\n",
    "        print(f'ERROR: No compatible file found for {site}')\n",
    "        continue\n",
    "    file = fluxnet_compatible_files[0]\n",
    "    data.append((site, os.path.join(INPUT_DIR, site, file), collection, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data/raw/icos-2023/unzipped/IT-Tor/ICOSETC_IT-Tor_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/FR-LGt/ICOSETC_FR-LGt_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/DE-Har/ICOSETC_DE-Har_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/CH-Dav/ICOSETC_CH-Dav_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/FI-Hyy/ICOSETC_FI-Hyy_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/IT-Ren/ICOSETC_IT-Ren_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/DE-Msr/ICOSETC_DE-Msr_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/FR-Lam/ICOSETC_FR-Lam_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/SE-Htm/ICOSETC_SE-Htm_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/SE-Svb/ICOSETC_SE-Svb_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/DE-Hai/ICOSETC_DE-Hai_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/FR-Bil/ICOSETC_FR-Bil_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/BE-Maa/ICOSETC_BE-Maa_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/FR-Fon/ICOSETC_FR-Fon_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/FR-Hes/ICOSETC_FR-Hes_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/GL-ZaH/ICOSETC_GL-ZaH_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/SE-Deg/ICOSETC_SE-Deg_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/DE-Kli/ICOSETC_DE-Kli_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/DE-Tha/ICOSETC_DE-Tha_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/GL-Dsk/ICOSETC_GL-Dsk_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/FI-Sii/ICOSETC_FI-Sii_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/DK-Sor/ICOSETC_DK-Sor_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/GL-NuF/ICOSETC_GL-NuF_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/FR-Tou/ICOSETC_FR-Tou_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/DE-RuW/ICOSETC_DE-RuW_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/IT-Lsn/ICOSETC_IT-Lsn_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/FR-Pue/ICOSETC_FR-Pue_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/DK-Vng/ICOSETC_DK-Vng_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/DE-Gri/ICOSETC_DE-Gri_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/IT-Niv/ICOSETC_IT-Niv_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/SE-Sto/ICOSETC_SE-Sto_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/CZ-wet/ICOSETC_CZ-wet_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/UK-AMo/ICOSETC_UK-AMo_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/DE-RuS/ICOSETC_DE-RuS_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/DE-RuR/ICOSETC_DE-RuR_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/CZ-BK1/ICOSETC_CZ-BK1_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/BE-Lon/ICOSETC_BE-Lon_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/SE-Nor/ICOSETC_SE-Nor_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/IT-Cp2/ICOSETC_IT-Cp2_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/FR-Gri/ICOSETC_FR-Gri_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/DK-Gds/ICOSETC_DK-Gds_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/IT-BFt/ICOSETC_IT-BFt_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/BE-Bra/ICOSETC_BE-Bra_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/CZ-Lnz/ICOSETC_CZ-Lnz_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/FR-EM2/ICOSETC_FR-EM2_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/DK-Skj/ICOSETC_DK-Skj_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/FI-Ken/ICOSETC_FI-Ken_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/BE-Lcr/ICOSETC_BE-Lcr_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/FI-Var/ICOSETC_FI-Var_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/IT-SR2/ICOSETC_IT-SR2_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/GF-Guy/ICOSETC_GF-Guy_FLUXNET_HH_L2.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2994/1648976969.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column] = np.nan\n",
      "/tmp/ipykernel_2994/1648976969.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column] = np.nan\n",
      "/tmp/ipykernel_2994/1648976969.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column] = np.nan\n",
      "/tmp/ipykernel_2994/1648976969.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column] = np.nan\n",
      "/tmp/ipykernel_2994/1648976969.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column] = np.nan\n",
      "/tmp/ipykernel_2994/1648976969.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column] = np.nan\n",
      "/tmp/ipykernel_2994/1648976969.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column] = np.nan\n",
      "/tmp/ipykernel_2994/1648976969.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column] = np.nan\n",
      "/tmp/ipykernel_2994/1648976969.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column] = np.nan\n",
      "/tmp/ipykernel_2994/1648976969.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column] = np.nan\n",
      "/tmp/ipykernel_2994/1648976969.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column] = np.nan\n",
      "/tmp/ipykernel_2994/1648976969.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column] = np.nan\n",
      "/tmp/ipykernel_2994/1648976969.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column] = np.nan\n",
      "/tmp/ipykernel_2994/1648976969.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column] = np.nan\n",
      "/tmp/ipykernel_2994/1648976969.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column] = np.nan\n",
      "/tmp/ipykernel_2994/1648976969.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column] = np.nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data/raw/icos-2023/unzipped/CD-Ygb/ICOSETC_CD-Ygb_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/DE-HoH/ICOSETC_DE-HoH_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/FR-Mej/ICOSETC_FR-Mej_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/BE-Vie/ICOSETC_BE-Vie_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/FR-Aur/ICOSETC_FR-Aur_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/DE-Geb/ICOSETC_DE-Geb_FLUXNET_HH_L2.csv...\n",
      "Processing data/raw/icos-2023/unzipped/FI-Let/ICOSETC_FI-Let_FLUXNET_HH_L2.csv...\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "for site, file, source, downsample in data:\n",
    "    print(f'Processing {file}...')\n",
    "    site_dir = os.path.join(OUTPUT_DIR, site)\n",
    "    if not os.path.exists(site_dir):\n",
    "        os.makedirs(site_dir)\n",
    "    \n",
    "    site_df = pd.read_csv(file)\n",
    "    processed_df = process_site_dataframe(site_df, downsample=downsample)\n",
    "    min_time = processed_df['TIMESTAMP_START'].min()\n",
    "    max_time = processed_df['TIMESTAMP_START'].max()\n",
    "    outfile = os.path.join(site_dir, f'{min_time}_{max_time}_{source}.csv')\n",
    "    processed_df.to_csv(outfile, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/intermediate/test_int_1/icos-2023/site_data.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.copyfile(META_FILE, os.path.join(OUTPUT_DIR, 'site_data.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from ecoperceiver import EcoPerceiverModel, EcoPerceiverConfig, EcoPerceiverDataset, ep_collate\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run = 'default'\n",
    "seed_checkpoints = [\n",
    "    ('seed_0', 6),\n",
    "    ('seed_10', 9),\n",
    "    ('seed_20', 10),\n",
    "    ('seed_30', 6),\n",
    "    ('seed_40', 6),\n",
    "    ('seed_50', 13),\n",
    "    ('seed_60', 8),\n",
    "    ('seed_70', 7),\n",
    "    ('seed_80', 8),\n",
    "    ('seed_90', 12),\n",
    "]\n",
    "\n",
    "DATA_DIR = Path('data') / 'carbonsense'\n",
    "RUN_DIR = Path('runs') / run\n",
    "CONFIG_PATH = RUN_DIR / 'config.yml'\n",
    "\n",
    "with open(CONFIG_PATH, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "TEST_SITES = config['data']['test_sites']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = EcoPerceiverDataset(\n",
    "    DATA_DIR, TEST_SITES,\n",
    "    context_length=config['model']['context_length'],\n",
    "    targets=config['data']['target_columns']\n",
    "    )\n",
    "\n",
    "data_loader_test = DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=128,\n",
    "    num_workers=config['data']['num_workers'], pin_memory=config['data']['pin_memory'],\n",
    "    collate_fn=ep_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config['model']['spectral_data_channels'] = dataset_test.num_channels()\n",
    "config['model']['tabular_inputs'] = dataset_test.columns()\n",
    "device = torch.device('cuda')\n",
    "model = EcoPerceiverModel(EcoPerceiverConfig(**config['model']))\n",
    "\n",
    "datatype = torch.float32\n",
    "cuda_major = torch.cuda.get_device_properties(device).major\n",
    "if cuda_major >= 8:\n",
    "    datatype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_df = dataset_test.get_target_dataframe()\n",
    "inference_df.set_index(['SITE_ID', 'timestamp'], inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_df = inference_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for seed, checkpoint in seed_checkpoints:\n",
    "    checkpoint_path = RUN_DIR / seed / f'checkpoint-{checkpoint}.pth'\n",
    "    results_path = RUN_DIR / seed / f'results-{checkpoint}.csv'\n",
    "    # if os.path.exists(results_path):\n",
    "    #     print(f'Already have results for {seed}-{checkpoint}, skipping...')\n",
    "    #     continue\n",
    "    \n",
    "    weights = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(weights['model'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(f'Running results for {seed}...')\n",
    "    for batch in tqdm(data_loader_test):\n",
    "        with torch.cuda.amp.autocast(dtype=datatype):\n",
    "            op = model(batch)\n",
    "            outputs = op['logits'].cpu().tolist()\n",
    "            # Update inference df\n",
    "            idx = pd.MultiIndex.from_tuples(zip(batch['site_ids'], batch['timestamps']), names=['SITE_ID', 'timestamp'])\n",
    "\n",
    "            inference_df.update(pd.DataFrame(outputs, columns=['Inferred'], index=idx))\n",
    "    inference_df.to_csv(results_path)\n",
    "    inference_df['Inferred'] = np.nan"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

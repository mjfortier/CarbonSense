{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('data')\n",
    "CLEANED_DIR = DATA_DIR / 'cleaned'\n",
    "MERGED_DIR = DATA_DIR / 'merged'\n",
    "CONFIG_FILE = Path('config.json')\n",
    "\n",
    "with open(CONFIG_FILE, 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "TIMESTAMP_COL = config['harmonize_columns']['timestamp_column']\n",
    "METEO_COLS = list(config['harmonize_columns']['meteorological_columns'].keys())\n",
    "SOIL_COLS = list(config['harmonize_columns']['soil_columns'].keys())\n",
    "FLUX_COLS = list(config['harmonize_columns']['flux_columns'].keys())\n",
    "ALL_COLS = METEO_COLS + SOIL_COLS + FLUX_COLS\n",
    "QC_COLS = [f'{c}_QC' for c in ALL_COLS]\n",
    "SOURCE_ORDER = {s: i for i, s in enumerate(config['combine_sources']['source_priority'])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataframes(df1, df2):\n",
    "    df_merged = pd.merge(df1,df2, on='timestamp', how='outer', suffixes=('_df1', '_df2'))\n",
    "    df_final = df_merged[['timestamp']].copy()\n",
    "    for col in ALL_COLS:\n",
    "        colqc = f'{col}_QC'\n",
    "        df_final[[col]] = np.nan\n",
    "        df_final[[colqc]] = np.nan\n",
    "        df1qc = df_merged[f'{colqc}_df1']\n",
    "        df2qc = df_merged[f'{colqc}_df2']\n",
    "\n",
    "        # Merging rules:\n",
    "        #   - For any row and any variable, use the one which has the lower QC value between the two sources\n",
    "        #   - If it's a tie, use the value from the newer source (assumed df1 for this function)\n",
    "        df1_copy_indices = ((df1qc.notna()) & (df2qc.isna())) | ((df1qc.notna()) & (df2qc.notna()) & (df1qc <= df2qc))\n",
    "        df_final.loc[df1_copy_indices, col] = df_merged.loc[df1_copy_indices, f'{col}_df1']\n",
    "        df_final.loc[df1_copy_indices, colqc] = df_merged.loc[df1_copy_indices, f'{colqc}_df1']\n",
    "        df2_copy_indices = ((df2qc.notna()) & (df1qc.isna())) | ((df2qc.notna()) & (df1qc.notna()) & (df2qc < df1qc))\n",
    "        df_final.loc[df2_copy_indices, col] = df_merged.loc[df2_copy_indices, f'{col}_df2']\n",
    "        df_final.loc[df2_copy_indices, colqc] = df_merged.loc[df2_copy_indices, f'{colqc}_df2']\n",
    "\n",
    "    return df_final.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "def merge_data(path):\n",
    "    files = [f for f in os.listdir(path) if f not in ['audit.json', 'meta.json']]\n",
    "    dfs = [(SOURCE_ORDER.get(f.split('_')[0], 9999), pd.read_csv(f'{path}/{f}')) for f in files]\n",
    "    dfs = [x[1] for x in sorted(dfs, key=lambda y: y[0])]\n",
    "    if len(dfs) == 1:\n",
    "        df = dfs[0]\n",
    "    else:\n",
    "        # Merge dataframes\n",
    "        df = dfs.pop(0)\n",
    "        while len(dfs) > 0:\n",
    "            df = merge_dataframes(df, dfs.pop(0))\n",
    "\n",
    "        # Fill missing timestamps. This prevents having multiple files per site for disjoint sources\n",
    "        df['timestamp'] = df['timestamp']\n",
    "        ts_col = pd.to_datetime(df['timestamp'], format='%Y%m%d%H%M')\n",
    "        timestamp_range = pd.date_range(start=ts_col.min(), end=ts_col.max(), freq='30T')\n",
    "        timestamp_range_int = timestamp_range.strftime('%Y%m%d%H%M').astype(int)\n",
    "        existing_timestamps = set(df['timestamp'])\n",
    "        missing_timestamps = timestamp_range_int[~timestamp_range_int.isin(existing_timestamps)]\n",
    "        if len(missing_timestamps) > 0:\n",
    "            print(path)\n",
    "            missing_data = {c: [np.nan for _ in range(len(missing_timestamps))] for c in ALL_COLS + QC_COLS}\n",
    "            missing_data['timestamp'] = missing_timestamps\n",
    "            missing_df = pd.DataFrame(missing_data)\n",
    "            df = pd.concat([df, missing_df], axis=0).sort_values('timestamp').reset_index(drop=True)\n",
    "    cols = list(df.columns)\n",
    "    ts_col = pd.to_datetime(df['timestamp'], format='%Y%m%d%H%M')\n",
    "    df['DOY'] = ts_col.dt.dayofyear.astype(float) - 1.0\n",
    "    df['TOD'] = ts_col.dt.hour.astype(float)\n",
    "    df['TOD'] += 0.5 * (ts_col.dt.minute.astype(float) == 30).astype(float)\n",
    "    df = df[['timestamp', 'DOY', 'TOD'] + cols[1:]]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 151/417 [10:34<11:43,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/cleaned/GL-NuF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 185/417 [12:58<18:35,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/cleaned/US-Ivo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 230/417 [15:04<08:31,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/cleaned/CZ-BK1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 242/417 [15:34<04:58,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/cleaned/FR-Hes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 258/417 [16:12<07:30,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/cleaned/FR-Pue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 294/417 [18:16<05:32,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/cleaned/US-Atq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 336/417 [20:12<03:52,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/cleaned/RU-Che\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 360/417 [21:32<03:14,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/cleaned/GL-ZaH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 365/417 [21:50<02:23,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/cleaned/CZ-Lnz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 417/417 [25:00<00:00,  3.60s/it]\n"
     ]
    }
   ],
   "source": [
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NpEncoder, self).default(obj)\n",
    "\n",
    "sites = os.listdir(CLEANED_DIR)\n",
    "for site in tqdm(sites):\n",
    "    df = merge_data(CLEANED_DIR / site)\n",
    "    \n",
    "    os.makedirs(MERGED_DIR / site, exist_ok=True)\n",
    "    shutil.copy(CLEANED_DIR / site / 'audit.json', MERGED_DIR / site / 'audit.json')\n",
    "\n",
    "    with open(CLEANED_DIR / site / 'meta.json', 'r') as f:\n",
    "        meta = json.loads(f.read())\n",
    "    meta['MIN_DATE'] = df['timestamp'][0]\n",
    "    meta['MAX_DATE'] = df['timestamp'][len(df)-1]\n",
    "    with open(MERGED_DIR / site / 'meta.json', 'w') as f:\n",
    "        f.write(json.dumps(meta,  cls=NpEncoder))\n",
    "    \n",
    "    df.to_csv(MERGED_DIR / site / 'data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

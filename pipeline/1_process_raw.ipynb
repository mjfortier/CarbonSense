{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('data')\n",
    "RENAMED_DIR = DATA_DIR / 'renamed'\n",
    "CLEANED_DIR = DATA_DIR / 'cleaned'\n",
    "CONFIG_FILE = Path('config.json')\n",
    "\n",
    "with open(CONFIG_FILE, 'r') as f:\n",
    "    config = json.load(f)['harmonize_columns']\n",
    "\n",
    "TIMESTAMP_COL = config['timestamp_column']\n",
    "PREDICTOR_COLS = config['predictor_columns']\n",
    "FLUX_COLS = config['flux_columns']\n",
    "DEFAULT_GAPFILL_QC_FLAGS = config['default_gapfill_qc_flags']\n",
    "\n",
    "META_FIELDS = ['SITE_ID', 'SITE_NAME', 'LOCATION_LON', 'LOCATION_LAT', 'LOCATION_ELEV', 'IGBP', 'MAT', 'MAP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: TS_F_MDS_1 & TS_F_MDS_1_QC are examples of variables being missed here.\n",
    "\n",
    "def process_site_dataframe(df):\n",
    "    df = df.replace(-9999.0, np.nan)\n",
    "    audit = {} # keep track of column sources\n",
    "    allowable_suffixes = DEFAULT_GAPFILL_QC_FLAGS.keys()\n",
    "\n",
    "    # We will copy all data into a new harmonized dataframe\n",
    "    h_df = df[[TIMESTAMP_COL]].copy()\n",
    "    h_df.rename(columns={TIMESTAMP_COL: 'timestamp'}, inplace=True)\n",
    "    audit['timestamp'] = [TIMESTAMP_COL]\n",
    "    \n",
    "    for col in PREDICTOR_COLS + FLUX_COLS:\n",
    "        colqc = f'{col}_QC'\n",
    "        audit[col] = []\n",
    "        audit[colqc] = []\n",
    "        \n",
    "        # Check if the column already exists verbatim\n",
    "        if col in df.columns:\n",
    "            h_df.loc[:, col] = df.loc[:, col]\n",
    "            audit[col].append(col)\n",
    "        else:\n",
    "            h_df.loc[:, col] = np.nan\n",
    "        \n",
    "        if colqc in df.columns:\n",
    "            h_df.loc[:, colqc] = df.loc[:, colqc]\n",
    "            audit[colqc].append(colqc)\n",
    "        else:\n",
    "            h_df.loc[:, colqc] = np.nan\n",
    "        \n",
    "        h_df.loc[h_df[col].notna(), colqc] = 0 # default for variables which do not have gapfilling suffixes\n",
    "        \n",
    "        # Check all possible suffix versions of the column (usually gapfilled)\n",
    "        for suffix in allowable_suffixes:\n",
    "            scol = col + suffix\n",
    "            scolqc = f'{scol}_QC'\n",
    "\n",
    "            if scol in df.columns:\n",
    "                audit[col].append(scol)\n",
    "\n",
    "                if scolqc in df.columns:\n",
    "                    fill_index = ((h_df[col].isna()) & (df[scol].notna())) | ((df[scol].notna()) & (h_df[colqc] > df[scolqc]))\n",
    "                    h_df.loc[fill_index, col] = df.loc[fill_index, scol]\n",
    "                    h_df.loc[fill_index, colqc] = df.loc[fill_index, scolqc]\n",
    "                    audit[colqc].append(scolqc)\n",
    "                else:\n",
    "                    fill_index = (h_df[col].isna()) & (df[scol].notna())\n",
    "                    h_df.loc[fill_index, col] = df.loc[fill_index, scol]\n",
    "                \n",
    "                # Fill remaining unknown QC index with default value\n",
    "                remaining_qc_ind = fill_index & (h_df[colqc].isna())\n",
    "                h_df.loc[remaining_qc_ind, colqc] = DEFAULT_GAPFILL_QC_FLAGS[suffix]\n",
    "\n",
    "    used_columns = [c for cs in audit.values() for c in cs]\n",
    "    audit['unused'] = [c for c in df.columns if c not in used_columns]\n",
    "    audit = {\n",
    "        'step': 'harmonize_columns',\n",
    "        'info': audit\n",
    "    }\n",
    "    return h_df, audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning site data...\n",
      "fluxnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [06:41<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ameriflux\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [06:40<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "icos-2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [01:42<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "icos-ww\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [06:27<00:00,  5.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fluxnet-ch4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [01:23<00:00,  1.06s/it]\n"
     ]
    }
   ],
   "source": [
    "sources = os.listdir(RENAMED_DIR)\n",
    "site_meta_dict = {}\n",
    "audit_dict = {}\n",
    "\n",
    "print('Cleaning site data...')\n",
    "for source in sources:\n",
    "    meta_df = pd.read_csv(RENAMED_DIR / source / 'site_data.csv')\n",
    "    site_csvs = [f for f in os.listdir(RENAMED_DIR / source) if f != 'site_data.csv']\n",
    "    print(source)\n",
    "    for csv in tqdm(site_csvs):\n",
    "        site = csv[:-4]\n",
    "        if site not in site_meta_dict.keys():\n",
    "            os.makedirs(CLEANED_DIR / site, exist_ok=True)\n",
    "            site_meta_dict[site] = {'COVERAGE': {}}\n",
    "            audit_dict[site] = []\n",
    "        \n",
    "        df = pd.read_csv(RENAMED_DIR / source / csv)\n",
    "        df, audit = process_site_dataframe(df)\n",
    "        time_min = df['timestamp'].min()\n",
    "        time_max = df['timestamp'].max()\n",
    "        df.to_csv(CLEANED_DIR / site / f'{source}_{time_min}_{time_max}.csv', index=False)\n",
    "\n",
    "        site_meta = meta_df[meta_df['SITE_ID'] == site].to_dict('records')[0]\n",
    "        site_meta = {k: v for k, v in site_meta.items() if k in META_FIELDS}\n",
    "        site_meta_dict[site].update(site_meta)\n",
    "        site_meta_dict[site]['COVERAGE'][source] = [time_min, time_max]\n",
    "        audit_dict[site].append(audit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing metadata and audit logs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 417/417 [00:00<00:00, 5370.35it/s]\n"
     ]
    }
   ],
   "source": [
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NpEncoder, self).default(obj)\n",
    "\n",
    "for s in site_meta_dict.keys():\n",
    "    for ss in site_meta_dict[s].keys():\n",
    "        if type(site_meta_dict[s][ss]) == float and math.isnan(site_meta_dict[s][ss]):\n",
    "            site_meta_dict[s][ss] = None\n",
    "\n",
    "print('Writing metadata and audit logs...')\n",
    "for site in tqdm(site_meta_dict.keys()):\n",
    "    with open(CLEANED_DIR / site / 'meta.json', 'w') as f:\n",
    "        f.write(json.dumps(site_meta_dict[site], cls=NpEncoder))\n",
    "    with open(CLEANED_DIR / site / 'audit.json', 'w') as f:\n",
    "        f.write(json.dumps({'harmonize_columns': audit_dict[site]}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

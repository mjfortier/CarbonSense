{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('data')\n",
    "RENAMED_DIR = DATA_DIR / 'renamed'\n",
    "CLEANED_DIR = DATA_DIR / 'cleaned'\n",
    "CONFIG_FILE = Path('config.json')\n",
    "\n",
    "with open(CONFIG_FILE, 'r') as f:\n",
    "    config = json.load(f)['harmonize_columns']\n",
    "\n",
    "TIMESTAMP_COL = config['timestamp_column']\n",
    "METEO_COLS = config['meteorological_columns']\n",
    "SOIL_COLS = config['soil_columns']\n",
    "FLUX_COLS = config['flux_columns']\n",
    "ALL_COLS = {}\n",
    "ALL_COLS.update(METEO_COLS)\n",
    "ALL_COLS.update(SOIL_COLS)\n",
    "ALL_COLS.update(FLUX_COLS)\n",
    "DEFAULT_GAPFILL_QC_FLAGS = config['default_gapfill_qc_flags']\n",
    "\n",
    "META_FIELDS = ['SITE_ID', 'SITE_NAME', 'LOCATION_LON', 'LOCATION_LAT', 'LOCATION_ELEV', 'IGBP', 'MAT', 'MAP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_qc_value(alias):\n",
    "    default = 0\n",
    "    for k, v in DEFAULT_GAPFILL_QC_FLAGS.items():\n",
    "        if k in alias and v > default:\n",
    "            default = v\n",
    "    return default\n",
    "\n",
    "\n",
    "def process_site_dataframe(df):\n",
    "    df = df.replace(-9999.0, np.nan)\n",
    "    audit = {} # keep track of column sources\n",
    "\n",
    "    # We will copy all data into a new harmonized dataframe\n",
    "    h_df = df[[TIMESTAMP_COL]].copy()\n",
    "    h_df.rename(columns={TIMESTAMP_COL: 'timestamp'}, inplace=True)\n",
    "    n_rows = h_df.shape[0]\n",
    "    audit['timestamp'] = [TIMESTAMP_COL]\n",
    "    \n",
    "    for col, col_aliases in ALL_COLS.items():\n",
    "        colqc = f'{col}_QC'\n",
    "        audit[col] = []\n",
    "\n",
    "        col_np = np.array([np.nan] * n_rows, dtype=float)\n",
    "        colqc_np = np.array([9999] * n_rows, dtype=float)\n",
    "\n",
    "        for alias in col_aliases:\n",
    "            if alias in df.columns:\n",
    "                alias_np = df[alias].values\n",
    "\n",
    "                aliasqc = f'{alias}_qc'\n",
    "                if aliasqc not in df.columns:\n",
    "                    aliasqc_np = np.array([default_qc_value(alias)] * n_rows, dtype=int)\n",
    "                else:\n",
    "                    aliasqc_np = np.nan_to_num(df[aliasqc].values, nan=default_qc_value(alias)).astype(int)\n",
    "                \n",
    "                fill_index = ((np.isnan(col_np)) & (~np.isnan(alias_np))) | ((~np.isnan(alias_np)) & (colqc_np > aliasqc_np))\n",
    "\n",
    "                col_np[fill_index] = alias_np[fill_index]\n",
    "                colqc_np[fill_index] = aliasqc_np[fill_index]\n",
    "                audit[col].append(alias)\n",
    "\n",
    "        colqc_np[colqc_np == 9999.0] = np.nan\n",
    "        h_df.loc[:,col] = col_np\n",
    "        h_df.loc[:,colqc] = colqc_np\n",
    "        \n",
    "    used_columns = [c for cs in audit.values() for c in cs]\n",
    "    audit['unused'] = [c for c in df.columns if c not in used_columns]\n",
    "    audit = {\n",
    "        'step': 'harmonize_columns',\n",
    "        'info': audit\n",
    "    }\n",
    "    return h_df, audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning site data...\n",
      "fluxnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 195/195 [09:24<00:00,  2.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ameriflux\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192/192 [10:23<00:00,  3.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "icos-2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58/58 [02:25<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "icos-ww\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [08:45<00:00,  7.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fluxnet-ch4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [01:49<00:00,  1.38s/it]\n"
     ]
    }
   ],
   "source": [
    "sources = os.listdir(RENAMED_DIR)\n",
    "site_meta_dict = {}\n",
    "audit_dict = {}\n",
    "print('Cleaning site data...')\n",
    "for source in sources:\n",
    "    meta_df = pd.read_csv(RENAMED_DIR / source / 'site_data.csv')\n",
    "    site_csvs = [f for f in os.listdir(RENAMED_DIR / source) if f != 'site_data.csv']\n",
    "    print(source)\n",
    "    for csv in tqdm(site_csvs):\n",
    "        site = csv[:-4]\n",
    "        if site not in site_meta_dict.keys():\n",
    "            os.makedirs(CLEANED_DIR / site, exist_ok=True)\n",
    "            site_meta_dict[site] = {'COVERAGE': {}}\n",
    "            audit_dict[site] = []\n",
    "        \n",
    "        df = pd.read_csv(RENAMED_DIR / source / csv)\n",
    "        df, audit = process_site_dataframe(df)\n",
    "        time_min = df['timestamp'].min()\n",
    "        time_max = df['timestamp'].max()\n",
    "        df.to_csv(CLEANED_DIR / site / f'{source}_{time_min}_{time_max}.csv', index=False)\n",
    "\n",
    "        site_meta = meta_df[meta_df['SITE_ID'] == site].to_dict('records')[0]\n",
    "        site_meta = {k: v for k, v in site_meta.items() if k in META_FIELDS}\n",
    "        site_meta_dict[site].update(site_meta)\n",
    "        site_meta_dict[site]['COVERAGE'][source] = [time_min, time_max]\n",
    "        audit_dict[site].append(audit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing metadata and audit logs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 417/417 [00:00<00:00, 5832.67it/s]\n"
     ]
    }
   ],
   "source": [
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NpEncoder, self).default(obj)\n",
    "\n",
    "for s in site_meta_dict.keys():\n",
    "    for ss in site_meta_dict[s].keys():\n",
    "        if type(site_meta_dict[s][ss]) == float and math.isnan(site_meta_dict[s][ss]):\n",
    "            site_meta_dict[s][ss] = None\n",
    "\n",
    "print('Writing metadata and audit logs...')\n",
    "for site in tqdm(site_meta_dict.keys()):\n",
    "    with open(CLEANED_DIR / site / 'meta.json', 'w') as f:\n",
    "        f.write(json.dumps(site_meta_dict[site], cls=NpEncoder))\n",
    "    with open(CLEANED_DIR / site / 'audit.json', 'w') as f:\n",
    "        f.write(json.dumps({'harmonize_columns': audit_dict[site]}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

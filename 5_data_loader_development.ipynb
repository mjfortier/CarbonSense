{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join('data', 'processed', 'v2')\n",
    "SITES = os.listdir(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FluxDataset(Dataset):\n",
    "    def __init__(self, data_dir, sites, context_length=48, targets=['GPP_NT_VUT_REF'], device='cpu'):\n",
    "        self.data_dir = data_dir\n",
    "        self.sites = sites\n",
    "        self.data = []\n",
    "        self.context_length = context_length\n",
    "        self.targets = targets\n",
    "        self.remove_columns = ['timestamp', 'NEE_VUT_REF', 'GPP_NT_VUT_REF', 'RECO_NT_VUT_REF']\n",
    "        self.device = device\n",
    "        \n",
    "        for root, _, files in os.walk(self.data_dir):\n",
    "            in_sites = False\n",
    "            for site in sites:\n",
    "                if site in root:\n",
    "                    in_sites = True\n",
    "            if not in_sites:\n",
    "                continue\n",
    "\n",
    "            if 'data.csv' in files:\n",
    "                df = pd.read_csv(os.path.join(root, 'data.csv'))\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "                with open(os.path.join(root, 'modis.pkl'), 'rb') as f:\n",
    "                    modis_data = pkl.load(f)\n",
    "                with open(os.path.join(root, 'meta.json'), 'r') as f:\n",
    "                    meta = json.load(f)\n",
    "\n",
    "                self.data.append((meta, df, modis_data))\n",
    "        \n",
    "        self.lookup_table = []\n",
    "        for i, d in enumerate(self.data):\n",
    "            _, df, _ = d\n",
    "            for r in range(self.context_length, len(df)+1):\n",
    "                self.lookup_table.append((i,r))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lookup_table)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        site_num, row_max = self.lookup_table[idx]\n",
    "        row_min = row_max - (self.context_length)\n",
    "\n",
    "        _, df, modis = self.data[site_num]\n",
    "        rows = df.iloc[row_min:row_max]\n",
    "        rows = rows.reset_index(drop=True)\n",
    "        modis_data = []\n",
    "        timestamps = list(rows['timestamp'])\n",
    "        for i, ts in enumerate(timestamps):\n",
    "            pixels = modis.get(ts, None)\n",
    "            if pixels is not None:\n",
    "                modis_data.append((i, torch.tensor(pixels[:,1:9,1:9]).to(self.device)))\n",
    "        \n",
    "        targets = torch.tensor(rows[self.targets].values).to(self.device)\n",
    "        row_values = torch.tensor(rows.drop(columns=self.remove_columns).values)\n",
    "        mask = row_values.isnan().to(self.device)\n",
    "        row_values = row_values.nan_to_num(-1.0).to(self.device) # just needs a numeric value, doesn't matter what\n",
    "\n",
    "        return row_values, mask, modis_data, targets\n",
    "\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    row_values, mask, modis_data, targets = zip(*batch)\n",
    "\n",
    "    # imgs are tensors with the same dim, can be stacked\n",
    "    row_values = torch.stack(row_values, dim=0)\n",
    "    mask = torch.stack(mask, dim=0)\n",
    "    targets = torch.stack(targets, dim=0)\n",
    "\n",
    "    # masks and classes have variable size per sample, so they get returned as a list\n",
    "    modis_data = [m for m in modis_data]\n",
    "\n",
    "    return row_values, mask, modis_data, targets\n",
    "\n",
    "def FluxDataLoader(data_dir, sites, context_length = 48, targets=['GPP_NT_VUT_REF'], device='cpu', **kwargs):\n",
    "    ds = FluxDataset(data_dir, sites, context_length=context_length, targets=targets, device=device)\n",
    "    return DataLoader(ds, collate_fn=custom_collate_fn, **kwargs)\n",
    "    \n",
    "\n",
    "dl = FluxDataLoader(DATA_DIR, SITES, batch_size=32, shuffle=True, num_workers=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, m, a, t = next(iter(dl))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

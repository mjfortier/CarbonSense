{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for experiments\n",
    "- Steps more specific to my architecture\n",
    "- Custom normalization, remove QC columns, index by date\n",
    "- Combine MODIS data and site metadata all into the same leaf directories (organize by time chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import json\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "int_2 should have merged datasets. Data is unnormalized.\n",
    "'''\n",
    "\n",
    "INTERMEDIATE_DIR = Path('data/intermediate/int_2')\n",
    "META_DIR = Path('data/meta')\n",
    "MODIS_A4_DIR = Path('data/modis_a4')\n",
    "MODIS_A2_DIR = Path('data/modis_a2')\n",
    "OUTPUT_DIR = Path('data/processed/v4')\n",
    "SITES = os.listdir(INTERMEDIATE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCD42A2 water cover map - binarize\n",
    "water_dict = {\n",
    "    0: 1, # shallow ocean\n",
    "    1: 0, # land\n",
    "    2: 0, # ocean coastlines and lake shorelines\n",
    "    3: 1, # shallow inland water\n",
    "    4: 1, # ephemeral water\n",
    "    5: 1, # deep inland water\n",
    "    6: 1, # moderate or continental ocean\n",
    "    7: 1, # deep ocean\n",
    "    255: 0 # fill value, treat as land for simplicity\n",
    "}\n",
    "\n",
    "# For all MODIS bands, we're treating -1 as a fill value\n",
    "def clean_a4_data(arr):\n",
    "    arr = np.where((arr > 30000) | (arr < 0), -10000, arr)\n",
    "    arr = np.where(arr > 10000, 10000, arr)\n",
    "    arr = arr / 10000.0\n",
    "    return arr.astype(np.float32)\n",
    "\n",
    "def clean_a2_data(arr):\n",
    "    # Snow: 0 = no snow, 1 = snow, 255 = fill\n",
    "    snow_arr = np.where((arr[0] == 255), -1, arr[0]).astype(np.float32)\n",
    "    water_arr = np.vectorize(water_dict.get)(arr[2]).astype(np.float32)\n",
    "    return np.stack((snow_arr, water_arr), axis=0)\n",
    "\n",
    "\n",
    "# Get modis and metadata\n",
    "modis_time = '12:00:00' # average of Terra and Aqua satelites\n",
    "df_meta = pd.read_csv(META_DIR / 'processed_site_meta.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input file list. We'll be using this a lot.\n",
    "csvs = []\n",
    "for site in SITES:\n",
    "    entries = os.listdir(os.path.join(INTERMEDIATE_DIR, site))\n",
    "    for e in entries:\n",
    "        csvs.append((os.path.join(INTERMEDIATE_DIR, site, e, 'data.csv'), site))\n",
    "\n",
    "with open('normalization_config.yml', 'r') as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tabular_data(df, config):\n",
    "    targets = [k for k, v in config.items() if v.get('target', False) and v.get('keep', False)]\n",
    "    predictors = [k for k, v in config.items() if not v.get('target', False) and v.get('keep', False)]\n",
    "\n",
    "    for p in predictors:\n",
    "        # Filter out gap fills based on config\n",
    "        max_qc = config[p].get('max_qc_flag', 2)\n",
    "        qc_flag = f'{p}_QC'\n",
    "        if qc_flag in df.columns:\n",
    "            df.loc[df[qc_flag] > max_qc, p] = np.nan\n",
    "\n",
    "        # Filter outlier values\n",
    "        max_v = config[p].get('normalization_max', None)\n",
    "        min_v = config[p].get('normalization_min', None)\n",
    "        df.loc[~df[p].between(min_v, max_v), p] = np.nan\n",
    "\n",
    "        # Min-max normalization\n",
    "        v_mid = (max_v + min_v) / 2\n",
    "        v_range = max_v - min_v\n",
    "        if config[p].get('cyclic', False):\n",
    "            v_range /= 2\n",
    "        df[p] = (df[p] - v_mid) / v_range\n",
    "\n",
    "    # Add timestamp index\n",
    "    df['timestamp'] = pd.to_datetime(df['TIMESTAMP_START'], format='%Y%m%d%H%M')\n",
    "\n",
    "    # Add day of year / time of day (and normalize as cyclic)\n",
    "    df['DOY'] = df['timestamp'].dt.dayofyear.astype(float) - 1.0\n",
    "    df['TOD'] = df['timestamp'].dt.hour.astype(float)\n",
    "    df['DOY'] = (df['DOY'] - 183) / 183\n",
    "    df['TOD'] = (df['TOD'] - 12) / 12\n",
    "\n",
    "    predictors = ['timestamp', 'DOY', 'TOD'] + predictors\n",
    "\n",
    "    for t in targets:\n",
    "        # Filter out gap fills based on config\n",
    "        max_qc = config[t].get('max_qc_flag', 2)\n",
    "        qc_flag = f'{t}_QC'\n",
    "        if qc_flag in df.columns:\n",
    "            df.loc[df[qc_flag] > max_qc, t] = np.nan\n",
    "\n",
    "    targets = ['timestamp'] + targets\n",
    "\n",
    "    df_p = df[predictors]\n",
    "    df_t = df[targets]\n",
    "    return df_p, df_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_modis_data(site, df):\n",
    "    with open(os.path.join(MODIS_A4_DIR, f'{site}.pkl'), 'rb') as f:\n",
    "        modis_a4_data = pkl.load(f)\n",
    "    modis_a4_pixels = modis_a4_data['pixel_values']\n",
    "    modis_a4_pixels = {pd.to_datetime(k, format='%Y_%m_%d').replace(hour=12): v for k, v in modis_a4_pixels.items()}\n",
    "    modis_a4_pixels = {k: v for k, v in modis_a4_pixels.items() if k in df['timestamp']}\n",
    "\n",
    "    with open(os.path.join(MODIS_A2_DIR, f'{site}.pkl'), 'rb') as f:\n",
    "        modis_a2_data = pkl.load(f)\n",
    "    modis_a2_pixels = modis_a2_data['pixel_values']\n",
    "    modis_a2_pixels = {pd.to_datetime(k, format='%Y_%m_%d').replace(hour=12): v for k, v in modis_a2_pixels.items()}\n",
    "    modis_a2_pixels = {k: v for k, v in modis_a2_pixels.items() if k in df['timestamp']}\n",
    "\n",
    "    # Normalize and join MODIS data\n",
    "    modis_data = {}\n",
    "    for k, a4_values in modis_a4_pixels.items():\n",
    "        a2_values = modis_a2_pixels.get(k, None)\n",
    "        if a2_values is None:\n",
    "            print(f'Mismatch in MODIS keys for {site}: {k}')\n",
    "            continue\n",
    "\n",
    "        clean_a4 = clean_a4_data(a4_values)\n",
    "        clean_a2 = clean_a2_data(a2_values)\n",
    "        if (np.sum(clean_a4 < 0) / len(clean_a4.flatten()) > 0.5):\n",
    "            continue\n",
    "        modis_data[k] = np.concatenate((clean_a4, clean_a2), axis=0)\n",
    "    return modis_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_site_meta(site, df_t, df_meta):\n",
    "    meta_row = df_meta.loc[df_meta['SITE_ID'] == site]\n",
    "    nan_to_null = lambda x: None if pd.isna(x) else x\n",
    "    meta = {\n",
    "        'SITE_ID': site,\n",
    "        'LOCATION_LAT': nan_to_null(meta_row['LOCATION_LAT'].values[0]),\n",
    "        'LOCATION_LON': nan_to_null(meta_row['LOCATION_LON'].values[0]),\n",
    "        'LOCATION_ELEV': nan_to_null(meta_row['LOCATION_ELEV'].values[0]),\n",
    "        'IGBP': nan_to_null(meta_row['IGBP'].values[0]),\n",
    "    }\n",
    "    timeinfo = ast.literal_eval(meta_row['TIME_INFO'].values[0])\n",
    "\n",
    "    timebound_string = ''\n",
    "    # find out which source we're dealing with\n",
    "    for source, bounds in timeinfo.items():\n",
    "        start = bounds[0].replace('_', '-')\n",
    "        end = bounds[1].replace('_', '-')\n",
    "        pd_dt = pd.to_datetime(start, format='%Y-%m-%d').replace(hour=12)\n",
    "        if pd_dt in df_t['timestamp']:\n",
    "            print('hi')\n",
    "            meta['SOURCES'] = source\n",
    "            meta['TIME'] = [start, end]\n",
    "            timebound_string = f'{start}_{end}'\n",
    "    \n",
    "    if len(timebound_string) == 0:\n",
    "        print(f'error with {site}...')\n",
    "        print(timeinfo)\n",
    "        print(start)\n",
    "        print(end)\n",
    "        print(type(pd_dt))\n",
    "        print(df_t['timestamp'].dtype)\n",
    "        print(pd_dt in df_t['timestamp'])\n",
    "        print(pd_dt)\n",
    "        print(df_t['timestamp'].iloc[:20])\n",
    "        raise Exception()\n",
    "    \n",
    "    return meta, timebound_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error with US-ORv...\n",
      "{'ameriflux,fluxnet': ['2011_01_01', '2016_12_31']}\n",
      "2011-01-01\n",
      "2016-12-31\n",
      "<class 'pandas._libs.tslibs.timestamps.Timestamp'>\n",
      "datetime64[ns]\n",
      "False\n",
      "2011-01-01 12:00:00\n",
      "0    2011-01-01 00:00:00\n",
      "1    2011-01-01 01:00:00\n",
      "2    2011-01-01 02:00:00\n",
      "3    2011-01-01 03:00:00\n",
      "4    2011-01-01 04:00:00\n",
      "5    2011-01-01 05:00:00\n",
      "6    2011-01-01 06:00:00\n",
      "7    2011-01-01 07:00:00\n",
      "8    2011-01-01 08:00:00\n",
      "9    2011-01-01 09:00:00\n",
      "10   2011-01-01 10:00:00\n",
      "11   2011-01-01 11:00:00\n",
      "12   2011-01-01 12:00:00\n",
      "13   2011-01-01 13:00:00\n",
      "14   2011-01-01 14:00:00\n",
      "15   2011-01-01 15:00:00\n",
      "16   2011-01-01 16:00:00\n",
      "17   2011-01-01 17:00:00\n",
      "18   2011-01-01 18:00:00\n",
      "19   2011-01-01 19:00:00\n",
      "Name: timestamp, dtype: datetime64[ns]\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m df_p, df_t \u001b[38;5;241m=\u001b[39m normalize_tabular_data(df, config)\n\u001b[1;32m      5\u001b[0m modis_data \u001b[38;5;241m=\u001b[39m normalize_modis_data(site, df_t)\n\u001b[0;32m----> 6\u001b[0m meta, time_string \u001b[38;5;241m=\u001b[39m \u001b[43mget_site_meta\u001b[49m\u001b[43m(\u001b[49m\u001b[43msite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_meta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Write it all out\u001b[39;00m\n\u001b[1;32m      9\u001b[0m site_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(OUTPUT_DIR, site)\n",
      "Cell \u001b[0;32mIn[21], line 35\u001b[0m, in \u001b[0;36mget_site_meta\u001b[0;34m(site, df_t, df_meta)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(pd_dt)\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(df_t[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[:\u001b[38;5;241m20\u001b[39m])\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m meta, timebound_string\n",
      "\u001b[0;31mException\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for file, site in csvs:\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    df_p, df_t = normalize_tabular_data(df, config)\n",
    "    modis_data = normalize_modis_data(site, df_t)\n",
    "    meta, time_string = get_site_meta(site, df_t, df_meta)\n",
    "\n",
    "    # Write it all out\n",
    "    site_dir = os.path.join(OUTPUT_DIR, site)\n",
    "    if not os.path.exists(site_dir):\n",
    "        os.makedirs(site_dir)\n",
    "    \n",
    "    section_dir = os.path.join(site_dir, time_string)\n",
    "    if not os.path.exists(section_dir):\n",
    "        os.makedirs(section_dir)\n",
    "    \n",
    "    df_t.to_csv(os.path.join(section_dir, 'targets.csv'))\n",
    "    df_p.to_csv(os.path.join(section_dir, 'predictors.csv'))\n",
    "    with open(os.path.join(section_dir, 'modis.pkl'), 'wb') as f:\n",
    "        pkl.dump(modis_data, f)\n",
    "    with open(os.path.join(section_dir, 'meta.json'), 'w') as f:\n",
    "        json.dump(meta, f)\n",
    "    \n",
    "    break\n",
    "\n",
    "# Histogram of values for a column\n",
    "# cols_to_examine = ['G_F_MDS', 'LW_IN_F', 'CO2_F_MDS', 'P_F']\n",
    "# big_df = pd.concat([d[cols_to_examine]for d in dfs], axis=0)\n",
    "# big_df['G_F_MDS'].hist(bins=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

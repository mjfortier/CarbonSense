{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for experiments\n",
    "- Steps more specific to my architecture\n",
    "- Custom normalization, remove QC columns, index by date\n",
    "- Combine MODIS data and site metadata all into the same leaf directories (organize by time chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "int_2 should have merged datasets. Data is unnormalized.\n",
    "'''\n",
    "\n",
    "DATA_DIR = os.path.join('data', 'intermediate', 'int_2')\n",
    "MODIS_A4_DIR = os.path.join('data', 'modis_a4')\n",
    "MODIS_A2_DIR = os.path.join('data', 'modis_a2')\n",
    "OUTPUT_DIR = os.path.join('data', 'processed', 'v3')\n",
    "SITES = os.listdir(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROP_COLS = ['PPFD_DIF', 'SW_DIF', 'PPFD_DIFF_QC', 'SW_DIF_QC']\n",
    "\n",
    "COL_THRESHOLDS = {\n",
    "    'TA_F': [-80.0, 80.0],\n",
    "    'SW_IN_F': [0, 2000.0],\n",
    "    'LW_IN_F': [0, 2000.0],\n",
    "    'VPD_F': [0.0, 110.0],\n",
    "    'PA_F': [20.0, 150.0],\n",
    "    'P_F': [0.0, 300.0],\n",
    "    'WS_F': [0.0, 100.0],\n",
    "    'WD': [0.0, 360.0],\n",
    "    'RH': [0.0, 100.0],\n",
    "    'USTAR': [0.0, 10.0],\n",
    "    'NETRAD': [-1000.0, 2000.0],\n",
    "    'PPFD_IN': [-200.0, 3000.0],\n",
    "    # 'PPFD_DIF': [],\n",
    "    'PPFD_OUT': [-200.0, 3000.0],\n",
    "    # 'SW_DIF': [],\n",
    "    'SW_OUT': [-100.0, 2000.0],\n",
    "    'LW_OUT': [-100.0, 2000.0],\n",
    "    'CO2_F_MDS': [0.0, 2000.0], # air CO2 concentration. Shouldn't be too far outside typical ranges.\n",
    "    'G_F_MDS': [-1000.0, 2000.0],\n",
    "    'LE_F_MDS': [-1000.0, 2000.0],\n",
    "    'H_F_MDS': [-1000.0, 2000.0],\n",
    "    'DOY': [0, 366],\n",
    "    'TOD': [0, 24],\n",
    "    # 'NEE_VUT_REF': [],\n",
    "    # 'RECO_NT_VUT_REF': [],\n",
    "    # 'GPP_NT_VUT_REF': []\n",
    "}\n",
    "\n",
    "PREDICT_COLS = ['NEE_VUT_REF', 'RECO_NT_VUT_REF', 'GPP_NT_VUT_REF']\n",
    "\n",
    "CYCLIC_COLS = ['WD', 'DOY', 'TOD'] # DOY = day of year, TOD = time of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input file list. We'll be using this a lot.\n",
    "csvs = []\n",
    "for site in SITES:\n",
    "    entries = os.listdir(os.path.join(DATA_DIR, site))\n",
    "    for e in entries:\n",
    "        csvs.append((os.path.join(DATA_DIR, site, e, 'data.csv'), site))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count lines and get min/max for each column\n",
    "col_minmax = {c: [None, None] for c in COL_THRESHOLDS.keys()}\n",
    "site_hours = 0\n",
    "cols_to_keep = list(COL_THRESHOLDS.keys()) + PREDICT_COLS\n",
    "\n",
    "data = []\n",
    "\n",
    "for file, site in csvs:\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    df['timestamp'] = pd.to_datetime(df['TIMESTAMP_START'], format='%Y%m%d%H%M')\n",
    "    df['DOY'] = df['timestamp'].dt.dayofyear.astype(float) - 1.0\n",
    "    df['TOD'] = df['timestamp'].dt.hour.astype(float)\n",
    "    df = df.set_index('timestamp')\n",
    "\n",
    "    for col, bounds in COL_THRESHOLDS.items():\n",
    "        low, high = bounds\n",
    "        col_qc = f'{col}_QC'\n",
    "        if col_qc in df.columns:\n",
    "            # Filter out bad interpolations\n",
    "            df.loc[df[col_qc]==3, col] = np.nan\n",
    "\n",
    "        # Filter outliers\n",
    "        df.loc[~df[col].between(low, high), col] = np.nan\n",
    "\n",
    "        # Normalize (kind of)\n",
    "        v_range = high - low\n",
    "        v_mid = (high + low) / 2\n",
    "        if col in CYCLIC_COLS:\n",
    "            v_range /= 2\n",
    "        df[col] = (df[col] - v_mid)/ v_range\n",
    "\n",
    "        # Get min/max of each attribute across the whole dataset\n",
    "        # cmin = df[c].min()\n",
    "        # cmax = df[c].max()\n",
    "        # if col_minmax[c][0] == None or col_minmax[c][0] > cmin:\n",
    "        #     col_minmax[c][0] = cmin\n",
    "        # if col_minmax[c][1] == None or col_minmax[c][1] < cmax:\n",
    "        #     col_minmax[c][1] = cmax\n",
    "\n",
    "    df = df[cols_to_keep]\n",
    "    data.append((df, site))\n",
    "    site_hours += len(df)\n",
    "\n",
    "# Histogram of values for a column\n",
    "# cols_to_examine = ['G_F_MDS', 'LW_IN_F', 'CO2_F_MDS', 'P_F']\n",
    "# big_df = pd.concat([d[cols_to_examine]for d in dfs], axis=0)\n",
    "# big_df['G_F_MDS'].hist(bins=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, I'm just binarizing it.\n",
    "water_dict = {\n",
    "    0: 1, # shallow ocean\n",
    "    1: 0, # land\n",
    "    2: 0, # ocean coastlines and lake shorelines\n",
    "    3: 1, # shallow inland water\n",
    "    4: 1, # ephemeral water\n",
    "    5: 1, # deep inland water\n",
    "    6: 1, # moderate or continental ocean\n",
    "    7: 1, # deep ocean\n",
    "    255: 0 # fill value, treat as land for simplicity\n",
    "}\n",
    "\n",
    "def clean_a4_data(arr):\n",
    "    arr = np.where((arr > 30000) | (arr < 0), -10000, arr)\n",
    "    arr = np.where(arr > 10000, 10000, arr)\n",
    "    arr = arr / 10000.0\n",
    "    return arr.astype(np.float32)\n",
    "\n",
    "def clean_a2_data(arr):\n",
    "    # Snow: 0 = no snow, 1 = snow, 255 = fill\n",
    "    snow_arr = np.where((arr[0] == 255), -1, arr[0]).astype(np.float32)\n",
    "    water_arr = np.vectorize(water_dict.get)(arr[2]).astype(np.float32)\n",
    "    return np.stack((snow_arr, water_arr), axis=0)\n",
    "\n",
    "\n",
    "# Get modis and metadata\n",
    "modis_time = '12:00:00' # average of Terra and Aqua satelites1\n",
    "df_meta = pd.read_csv('processed_site_meta.csv')\n",
    "\n",
    "for df, site in data:\n",
    "    with open(os.path.join(MODIS_A4_DIR, f'{site}.pkl'), 'rb') as f:\n",
    "        modis_a4_data = pkl.load(f)\n",
    "    modis_a4_pixels = modis_a4_data['pixel_values']\n",
    "    modis_a4_pixels = {pd.to_datetime(k, format='%Y_%m_%d').replace(hour=12): v for k, v in modis_a4_pixels.items()}\n",
    "    modis_a4_pixels = {k: v for k, v in modis_a4_pixels.items() if k in df.index}\n",
    "\n",
    "    with open(os.path.join(MODIS_A2_DIR, f'{site}.pkl'), 'rb') as f:\n",
    "        modis_a2_data = pkl.load(f)\n",
    "    modis_a2_pixels = modis_a2_data['pixel_values']\n",
    "    modis_a2_pixels = {pd.to_datetime(k, format='%Y_%m_%d').replace(hour=12): v for k, v in modis_a2_pixels.items()}\n",
    "    modis_a2_pixels = {k: v for k, v in modis_a2_pixels.items() if k in df.index}\n",
    "\n",
    "    # Normalize and join MODIS data\n",
    "    modis_data = {}\n",
    "    for k, a4_values in modis_a4_pixels.items():\n",
    "        a2_values = modis_a2_pixels.get(k, None)\n",
    "        if a2_values is None:\n",
    "            print(f'Mismatch in MODIS keys for {site}: {k}')\n",
    "            continue\n",
    "\n",
    "        clean_a4 = clean_a4_data(a4_values)\n",
    "        clean_a2 = clean_a2_data(a2_values)\n",
    "        if (np.sum(clean_a4 < 0) / len(clean_a4.flatten()) > 0.5):\n",
    "            continue\n",
    "        modis_data[k] = np.concatenate((clean_a4, clean_a2), axis=0)\n",
    "\n",
    "\n",
    "    meta_row = df_meta.loc[df_meta['SITE_ID'] == site]\n",
    "    nan_to_null = lambda x: None if pd.isna(x) else x\n",
    "    meta = {\n",
    "        'SITE_ID': site,\n",
    "        'LOCATION_LAT': nan_to_null(meta_row['LOCATION_LAT'].values[0]),\n",
    "        'LOCATION_LON': nan_to_null(meta_row['LOCATION_LON'].values[0]),\n",
    "        'LOCATION_ELEV': nan_to_null(meta_row['LOCATION_ELEV'].values[0]),\n",
    "        'IGBP': nan_to_null(meta_row['IGBP'].values[0]),\n",
    "    }\n",
    "    timeinfo = ast.literal_eval(meta_row['TIME_INFO'].values[0])\n",
    "\n",
    "    timebound_string = ''\n",
    "    # find out which source we're dealing with\n",
    "    for source, bounds in timeinfo.items():\n",
    "        start = bounds[0].replace('_', '-')\n",
    "        end = bounds[1].replace('_', '-')\n",
    "        pd_dt = pd.to_datetime(start, format='%Y-%m-%d').replace(hour=12)\n",
    "        if pd_dt in df.index:\n",
    "            meta['SOURCES'] = source\n",
    "            meta['TIME'] = [start, end]\n",
    "            timebound_string = f'{start}_{end}'\n",
    "    \n",
    "    if len(timebound_string) == 0:\n",
    "        print(f'error with {site}...')\n",
    "        print(timeinfo)\n",
    "        print(start)\n",
    "        print(end)\n",
    "        print(df)\n",
    "        break\n",
    "        \n",
    "    # Write it all out\n",
    "    site_dir = os.path.join(OUTPUT_DIR, site)\n",
    "    if not os.path.exists(site_dir):\n",
    "        os.makedirs(site_dir)\n",
    "    \n",
    "    section_dir = os.path.join(site_dir, timebound_string)\n",
    "    if not os.path.exists(section_dir):\n",
    "        os.makedirs(section_dir)\n",
    "    \n",
    "    df.to_csv(os.path.join(section_dir, 'data.csv'))\n",
    "    with open(os.path.join(section_dir, 'modis.pkl'), 'wb') as f:\n",
    "        pkl.dump(modis_data, f)\n",
    "    with open(os.path.join(section_dir, 'meta.json'), 'w') as f:\n",
    "        json.dump(meta, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize MODIS data\n",
    "- Fill value is 32767, and valid range is 0-32766"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Examine distribution of MODIS data\n",
    "# modis_files = []\n",
    "# for root, dirs, files in os.walk(OUTPUT_DIR):\n",
    "#     if 'modis.pkl' in files:\n",
    "#         modis_files.append(os.path.join(root, 'modis.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discarded 127455 out of 391 images\n"
     ]
    }
   ],
   "source": [
    "# def clean_array(arr):\n",
    "#     arr = np.where((arr > 30000) | (arr < 0), -10000, arr)\n",
    "#     arr = np.where(arr > 10000, 10000, arr)\n",
    "#     arr = arr / 10000.0\n",
    "#     return arr\n",
    "\n",
    "# for file in modis_files:\n",
    "#     with open(file, 'rb') as f:\n",
    "#         modis_data = pkl.load(f)\n",
    "    \n",
    "#     cleaned_pixel_values = {}\n",
    "#     for k,v in modis_data.items():\n",
    "#         clean = clean_array(v)\n",
    "#         if np.sum(clean < 0) / len(clean.flatten()) <= 0.5:\n",
    "#             cleaned_pixel_values[k] = clean\n",
    "\n",
    "#     with open(file, 'wb') as f:\n",
    "#         pkl.dump(cleaned_pixel_values, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

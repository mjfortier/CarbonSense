{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for experiments\n",
    "- Steps more specific to my architecture\n",
    "- Custom normalization, remove QC columns, index by date\n",
    "- Combine MODIS data and site metadata all into the same leaf directories (organize by time chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERMEDIATE_DIR_2 = Path('data/intermediate/int_2')\n",
    "SITES = os.listdir(INTERMEDIATE_DIR_2)\n",
    "META_DIR = Path('data/meta')\n",
    "MODIS_A4_DIR = Path('data/modis_a4')\n",
    "MODIS_A2_DIR = Path('data/modis_a2')\n",
    "OUTPUT_DIR = Path('data/processed/CarbonSense')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCD42A2 water cover map - binarize\n",
    "water_dict = {\n",
    "    0: 1, # shallow ocean\n",
    "    1: 0, # land\n",
    "    2: 0, # ocean coastlines and lake shorelines\n",
    "    3: 1, # shallow inland water\n",
    "    4: 1, # ephemeral water\n",
    "    5: 1, # deep inland water\n",
    "    6: 1, # moderate or continental ocean\n",
    "    7: 1, # deep ocean\n",
    "    255: 0 # fill value, treat as land for simplicity\n",
    "}\n",
    "\n",
    "# For all MODIS bands, we're treating -1 as a fill value\n",
    "def clean_a4_data(arr):\n",
    "    arr = np.where((arr > 30000) | (arr < 0), -10000, arr)\n",
    "    arr = np.where(arr > 10000, 10000, arr)\n",
    "    arr = arr / 10000.0\n",
    "    return arr.astype(np.float32)\n",
    "\n",
    "def clean_a2_data(arr):\n",
    "    # Snow: 0 = no snow, 1 = snow, 255 = fill\n",
    "    snow_arr = np.where((arr[0] == 255), -1, arr[0]).astype(np.float32)\n",
    "    water_arr = np.vectorize(water_dict.get)(arr[2]).astype(np.float32)\n",
    "    return np.stack((snow_arr, water_arr), axis=0)\n",
    "\n",
    "\n",
    "# Get modis and metadata\n",
    "modis_time = '12:00:00' # average of Terra and Aqua satelites\n",
    "df_meta = pd.read_csv(META_DIR / 'processed_site_meta.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input file list. We'll be using this a lot.\n",
    "csvs = []\n",
    "for site in SITES:\n",
    "    entries = os.listdir(os.path.join(INTERMEDIATE_DIR_2, site))\n",
    "    for e in entries:\n",
    "        csvs.append((os.path.join(INTERMEDIATE_DIR_2, site, e, 'data.csv'), site))\n",
    "\n",
    "with open('normalization_config.yml', 'r') as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tabular_data(df, config):\n",
    "    targets = [k for k, v in config.items() if v.get('target', False) and v.get('keep', False)]\n",
    "    predictors = [k for k, v in config.items() if not v.get('target', False) and v.get('keep', False)]\n",
    "\n",
    "    for p in predictors:\n",
    "        # Filter out gap fills based on config\n",
    "        max_qc = config[p].get('max_qc_flag', 2)\n",
    "        qc_flag = f'{p}_QC'\n",
    "        if qc_flag in df.columns:\n",
    "            df.loc[df[qc_flag] > max_qc, p] = np.nan\n",
    "\n",
    "        # Filter outlier values\n",
    "        max_v = config[p].get('normalization_max', None)\n",
    "        min_v = config[p].get('normalization_min', None)\n",
    "        df.loc[~df[p].between(min_v, max_v), p] = np.nan\n",
    "\n",
    "        # Min-max normalization\n",
    "        v_mid = (max_v + min_v) / 2\n",
    "        v_range = max_v - min_v\n",
    "        if config[p].get('cyclic', False):\n",
    "            v_range /= 2\n",
    "        df[p] = (df[p] - v_mid) / v_range\n",
    "\n",
    "    # Add timestamp index\n",
    "    df['timestamp'] = pd.to_datetime(df['TIMESTAMP_START'], format='%Y%m%d%H%M')\n",
    "\n",
    "    # Add day of year / time of day (and normalize as cyclic)\n",
    "    df['DOY'] = df['timestamp'].dt.dayofyear.astype(float) - 1.0\n",
    "    df['TOD'] = df['timestamp'].dt.hour.astype(float)\n",
    "    df['DOY'] = (df['DOY'] - 183) / 183\n",
    "    df['TOD'] = (df['TOD'] - 12) / 12\n",
    "\n",
    "    predictors = ['timestamp', 'DOY', 'TOD'] + predictors\n",
    "\n",
    "    for t in targets:\n",
    "        # Filter out gap fills based on config\n",
    "        max_qc = config[t].get('max_qc_flag', 2)\n",
    "        qc_flag = f'{t}_QC'\n",
    "        if qc_flag in df.columns:\n",
    "            df.loc[df[qc_flag] > max_qc, t] = np.nan\n",
    "\n",
    "    targets = ['timestamp'] + targets\n",
    "\n",
    "    df_p = df[predictors]\n",
    "    df_p = df_p.set_index('timestamp', drop=True)\n",
    "    df_t = df[targets]\n",
    "    df_t = df_t.set_index('timestamp', drop=True)\n",
    "    return df_p, df_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_modis_data(site, df):\n",
    "    if not os.path.exists(MODIS_A4_DIR / f'{site}.pkl'):\n",
    "        return None\n",
    "    with open(MODIS_A4_DIR / f'{site}.pkl', 'rb') as f:\n",
    "        modis_a4_data = pkl.load(f)\n",
    "    modis_a4_pixels = modis_a4_data['pixel_values']\n",
    "    modis_a4_pixels = {pd.to_datetime(k, format='%Y_%m_%d').replace(hour=12): v for k, v in modis_a4_pixels.items()}\n",
    "    modis_a4_pixels = {k: v for k, v in modis_a4_pixels.items() if k in df.index}\n",
    "\n",
    "    with open(os.path.join(MODIS_A2_DIR, f'{site}.pkl'), 'rb') as f:\n",
    "        modis_a2_data = pkl.load(f)\n",
    "    modis_a2_pixels = modis_a2_data['pixel_values']\n",
    "    modis_a2_pixels = {pd.to_datetime(k, format='%Y_%m_%d').replace(hour=12): v for k, v in modis_a2_pixels.items()}\n",
    "    modis_a2_pixels = {k: v for k, v in modis_a2_pixels.items() if k in df.index}\n",
    "\n",
    "    # Normalize and join MODIS data\n",
    "    modis_data = {}\n",
    "    for k, a4_values in modis_a4_pixels.items():\n",
    "        a2_values = modis_a2_pixels.get(k, None)\n",
    "        if a2_values is None:\n",
    "            print(f'Mismatch in MODIS keys for {site}: {k}')\n",
    "            continue\n",
    "\n",
    "        clean_a4 = clean_a4_data(a4_values)\n",
    "        clean_a2 = clean_a2_data(a2_values)\n",
    "        if (np.sum(clean_a4 < 0) / len(clean_a4.flatten()) > 0.5):\n",
    "            continue\n",
    "        modis_data[k] = np.concatenate((clean_a4, clean_a2), axis=0)\n",
    "    return modis_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_site_meta(site, df_t, df_meta):\n",
    "    meta_row = df_meta.loc[df_meta['SITE_ID'] == site]\n",
    "    nan_to_null = lambda x: None if pd.isna(x) else x\n",
    "    meta = {\n",
    "        'SITE_ID': site,\n",
    "        'LOCATION_LAT': nan_to_null(meta_row['LOCATION_LAT'].values[0]),\n",
    "        'LOCATION_LON': nan_to_null(meta_row['LOCATION_LON'].values[0]),\n",
    "        'LOCATION_ELEV': nan_to_null(meta_row['LOCATION_ELEV'].values[0]),\n",
    "        'IGBP': nan_to_null(meta_row['IGBP'].values[0]),\n",
    "    }\n",
    "    timeinfo = ast.literal_eval(meta_row['TIME_INFO'].values[0])\n",
    "\n",
    "    timebound_string = ''\n",
    "    # find out which source we're dealing with\n",
    "    for source, bounds in timeinfo.items():\n",
    "        start = bounds[0].replace('_', '-')\n",
    "        end = bounds[1].replace('_', '-')\n",
    "        pd_dt = pd.to_datetime(start, format='%Y-%m-%d').replace(hour=12)\n",
    "        if pd_dt in df_t.index:\n",
    "            meta['SOURCES'] = source\n",
    "            meta['TIME'] = [start, end]\n",
    "            timebound_string = f'{start}_{end}'\n",
    "    \n",
    "    if len(timebound_string) == 0:\n",
    "        print(f'error with {site}...')\n",
    "        print(timeinfo)\n",
    "        print(start)\n",
    "        print(end)\n",
    "        raise Exception()\n",
    "    \n",
    "    return meta, timebound_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 226/392 [06:37<02:34,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: No MODIS data found for US-LWW, discarding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 392/392 [10:54<00:00,  1.67s/it]\n"
     ]
    }
   ],
   "source": [
    "for file, site in tqdm(csvs):\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    df_p, df_t = normalize_tabular_data(df, config)\n",
    "    modis_data = normalize_modis_data(site, df_t)\n",
    "    if modis_data is None:\n",
    "        print(f'WARNING: No MODIS data found for {site}, discarding.')\n",
    "        continue\n",
    "    meta, time_string = get_site_meta(site, df_t, df_meta)\n",
    "\n",
    "    # Write it all out\n",
    "    site_dir = os.path.join(OUTPUT_DIR, site)\n",
    "    if not os.path.exists(site_dir):\n",
    "        os.makedirs(site_dir)\n",
    "    \n",
    "    section_dir = os.path.join(site_dir, time_string)\n",
    "    if not os.path.exists(section_dir):\n",
    "        os.makedirs(section_dir)\n",
    "    \n",
    "    df_t.to_csv(os.path.join(section_dir, 'targets.csv'))\n",
    "    df_p.to_csv(os.path.join(section_dir, 'predictors.csv'))\n",
    "    with open(os.path.join(section_dir, 'modis.pkl'), 'wb') as f:\n",
    "        pkl.dump(modis_data, f)\n",
    "    with open(os.path.join(section_dir, 'meta.json'), 'w') as f:\n",
    "        json.dump(meta, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
